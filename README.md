# Gemma3-1b å·¥å…·è°ƒç”¨å¾®è°ƒé¡¹ç›®

ä½¿ç”¨PEFTï¼ˆParameter Efficient Fine-Tuningï¼‰å¾®è°ƒGemma3-1bæ¨¡å‹ï¼Œä½¿å…¶æ”¯æŒå·¥å…·è°ƒç”¨åŠŸèƒ½ã€‚

## é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®ä½¿ç”¨shawhin/tool-use-finetuningæ•°æ®é›†å¯¹Googleçš„Gemma3-1b-itæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œé€šè¿‡LoRAï¼ˆLow-Rank Adaptationï¼‰æ–¹æ³•å®ç°å‚æ•°é«˜æ•ˆçš„å¾®è°ƒï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿç†è§£å’Œæ‰§è¡Œå·¥å…·è°ƒç”¨ä»»åŠ¡ã€‚

## ğŸš€ åŠ é€Ÿæ–¹æ¡ˆ

### 1. Hugging Face Jobs (æ¨è)
ä½¿ç”¨Hugging Face Jobsè¿›è¡Œäº‘ç«¯å¾®è°ƒï¼Œæ— éœ€æœ¬åœ°GPUï¼š

```bash
# å®‰è£…ä¾èµ–
pip install huggingface_hub

# è®¾ç½®ç¯å¢ƒå˜é‡
export HF_TOKEN="your_huggingface_token"

# è¿è¡Œå¾®è°ƒ
python scripts/train_hf_jobs.py --flavor a10g-small
```

**ç¡¬ä»¶é€‰æ‹©**:
- `t4-small`: $0.40/å°æ—¶ (16GB GPU)
- `a10g-small`: $1.00/å°æ—¶ (24GB GPU) 
- `a10g-large`: $1.50/å°æ—¶ (24GB GPU)
- `a100-large`: $4.00/å°æ—¶ (80GB GPU)

### 2. Hugging Face Spaces
åˆ›å»ºSpaceè¿›è¡Œå¾®è°ƒï¼š

1. å°†æ­¤é¡¹ç›®æ¨é€åˆ°Hugging Face Hub
2. åˆ›å»ºæ–°çš„Spaceï¼Œé€‰æ‹©Gradioæ¨¡æ¿
3. å‡çº§åˆ°GPUç¡¬ä»¶
4. è¿è¡Œå¾®è°ƒ

### 3. æœ¬åœ°è®­ç»ƒ
å¦‚æœæ‚¨æœ‰å¼ºå¤§çš„æœ¬åœ°GPUï¼š

```bash
# å®‰è£…ä¾èµ–
uv sync

# è¿è¡Œè®­ç»ƒ
python scripts/train.py
```

## ç¯å¢ƒè¦æ±‚

- Python >= 3.9
- CUDAå…¼å®¹çš„GPUï¼ˆæ¨è16GB+æ˜¾å­˜ï¼‰
- UVåŒ…ç®¡ç†å™¨

## å®‰è£…

1. å…‹éš†é¡¹ç›®ï¼š
```bash
git clone <your-repo-url>
cd gemma3-tool-finetuning
```

2. ä½¿ç”¨UVå®‰è£…ä¾èµ–ï¼š
```bash
uv sync
```

## é¡¹ç›®ç»“æ„

```
gemma3-tool-finetuning/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_processor.py      # æ•°æ®å¤„ç†æ¨¡å—
â”‚   â”œâ”€â”€ model_config.py        # æ¨¡å‹é…ç½®
â”‚   â”œâ”€â”€ trainer.py             # è®­ç»ƒå™¨
â”‚   â””â”€â”€ utils.py               # å·¥å…·å‡½æ•°
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ training_config.yaml   # è®­ç»ƒé…ç½®
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ prepare_data.py        # æ•°æ®å‡†å¤‡è„šæœ¬
â”‚   â”œâ”€â”€ train.py               # è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ train_hf_jobs.py       # HF Jobsè®­ç»ƒè„šæœ¬
â”‚   â””â”€â”€ evaluate.py            # è¯„ä¼°è„šæœ¬
â”œâ”€â”€ tests/                     # æµ‹è¯•æ–‡ä»¶
â”œâ”€â”€ app.py                     # Spaceåº”ç”¨å…¥å£
â”œâ”€â”€ pyproject.toml            # é¡¹ç›®é…ç½®
â””â”€â”€ README.md                 # é¡¹ç›®æ–‡æ¡£
```

## ä½¿ç”¨æ–¹æ³•

### ğŸš€ å¿«é€Ÿå¯åŠ¨

```bash
# æœ¬åœ°è®­ç»ƒ
python scripts/quick_start.py --mode local

# Hugging Face Jobsè®­ç»ƒ
export HF_TOKEN="your_token"
python scripts/quick_start.py --mode hf-jobs --flavor a10g-small

# Hugging Face Spacesè®­ç»ƒ
python scripts/quick_start.py --mode hf-spaces
```

### è¯¦ç»†æ­¥éª¤

#### 1. æ•°æ®å‡†å¤‡

```bash
python scripts/prepare_data.py
```

#### 2. å¼€å§‹è®­ç»ƒ

##### æœ¬åœ°è®­ç»ƒ
```bash
python scripts/train.py --config configs/training_config.yaml
```

##### Hugging Face Jobsè®­ç»ƒ
```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export HF_TOKEN="your_huggingface_token"

# è¿è¡Œå¾®è°ƒ
python scripts/train_hf_jobs.py --flavor a10g-small
```

##### Spaceè®­ç»ƒ
1. éƒ¨ç½²åˆ°Hugging Face Spaces
2. å‡çº§åˆ°GPUç¡¬ä»¶
3. é€šè¿‡Webç•Œé¢å¯åŠ¨è®­ç»ƒ

### 3. è¯„ä¼°æ¨¡å‹

```bash
python scripts/evaluate.py --model_path ./outputs/checkpoint-final
```

## é…ç½®è¯´æ˜

ä¸»è¦é…ç½®å‚æ•°åœ¨`configs/training_config.yaml`ä¸­ï¼š

- `model_name`: åŸºç¡€æ¨¡å‹åç§°ï¼ˆgoogle/gemma-3-1b-itï¼‰
- `dataset_name`: æ•°æ®é›†åç§°ï¼ˆshawhin/tool-use-finetuningï¼‰
- `lora_config`: LoRAé…ç½®å‚æ•°
- `training_args`: è®­ç»ƒå‚æ•°

## æŠ€æœ¯æ ˆ

- **æ¨¡å‹**: Google Gemma3-1b-it
- **å¾®è°ƒæ–¹æ³•**: PEFT LoRA
- **æ•°æ®é›†**: shawhin/tool-use-finetuning
- **æ¡†æ¶**: Transformers, PyTorch
- **åŒ…ç®¡ç†**: UV
- **åŠ é€Ÿå¹³å°**: Hugging Face Jobs/Spaces

## æˆæœ¬ä¼°ç®—

ä½¿ç”¨Hugging Face Jobsçš„é¢„ä¼°æˆæœ¬ï¼š

| ç¡¬ä»¶é…ç½® | æ¯å°æ—¶ä»·æ ¼ | 3å°æ—¶è®­ç»ƒ | 10å°æ—¶è®­ç»ƒ |
|---------|-----------|-----------|------------|
| T4 Small | $0.40 | $1.20 | $4.00 |
| A10G Small | $1.00 | $3.00 | $10.00 |
| A10G Large | $1.50 | $4.50 | $15.00 |
| A100 Large | $4.00 | $12.00 | $40.00 |

## è®¸å¯è¯

MIT License

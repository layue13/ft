# Gemma-3-1b Tool Use å¾®è°ƒé¡¹ç›®

åŸºäºç¬¬ä¸€æ€§åŸç†çš„Gemma-3-1bå·¥å…·è°ƒç”¨å¾®è°ƒé¡¹ç›®ï¼Œè®©æ¨¡å‹å­¦ä¼šä½¿ç”¨å·¥å…·ã€‚

## ğŸ¯ é¡¹ç›®ç›®æ ‡

- å¾®è°ƒGemma-3-1bæ¨¡å‹ä»¥æ”¯æŒå·¥å…·è°ƒç”¨
- ä½¿ç”¨çœŸå®çš„å·¥å…·è°ƒç”¨æ•°æ®é›†è¿›è¡Œè®­ç»ƒ
- åŸºäºLoRAæŠ€æœ¯è¿›è¡Œé«˜æ•ˆå¾®è°ƒ
- **ä½¿ç”¨uvè¿›è¡Œä¾èµ–ç®¡ç†**

## ğŸ“¦ ä¾èµ–ç®¡ç†

æœ¬é¡¹ç›®ä½¿ç”¨ `uv` è¿›è¡Œä¾èµ–ç®¡ç†ï¼š

```bash
# å®‰è£…ä¾èµ–
uv sync

# è¿è¡Œè®­ç»ƒè„šæœ¬
uv run python simple_train.py
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æœ¬åœ°è®­ç»ƒ

1. å…‹éš†é¡¹ç›®
```bash
git clone https://github.com/layue13/ft.git
cd ft
```

2. å®‰è£…ä¾èµ–
```bash
uv sync
```

3. è¿è¡Œè®­ç»ƒ
```bash
uv run python simple_train.py
```

### äº‘ç«¯è®­ç»ƒ (HF Jobs)

```bash
# ä¸€é”®éƒ¨ç½²åˆ°HF Jobs
hf jobs run --flavor a100-40gb --secrets HF_TOKEN pytorch/pytorch:2.6.0-cuda12.4-cudnn9-devel bash -c "
pip install uv &&
git clone https://github.com/layue13/ft.git &&
cd ft &&
uv sync &&
uv run python hf_jobs_train.py
"
```

## ğŸ“Š é¡¹ç›®ç‰¹æ€§

| ç‰¹æ€§ | è¯´æ˜ |
|------|------|
| **æ¨¡å‹** | google/gemma-3-1b-it |
| **æ•°æ®é›†** | shawhin/tool-use-finetuning (477ä¸ªæ ·æœ¬) |
| **å¾®è°ƒæ–¹æ³•** | LoRA (Low-Rank Adaptation) |
| **ä¾èµ–ç®¡ç†** | uv |
| **è®­ç»ƒæ—¶é—´** | 1-2å°æ—¶ (A100) |
| **é¢„æœŸæˆæœ¬** | $2-4 (HF Jobs) |

## ğŸ”§ æŠ€æœ¯æ ˆ

- **æ¨¡å‹**: Gemma-3-1b-it
- **å¾®è°ƒæ–¹æ³•**: LoRA (r=16, alpha=32)
- **æ¡†æ¶**: Transformers + PEFT
- **æ•°æ®æ ¼å¼**: Gemmaå¯¹è¯æ ¼å¼
- **ä¼˜åŒ–å™¨**: AdamW (lr=2e-5)
- **ç²¾åº¦**: bfloat16 (GPU) / float32 (CPU)
- **ä¾èµ–ç®¡ç†**: uv

## ğŸ“ é¡¹ç›®ç»“æ„

```
ft/
â”œâ”€â”€ simple_train.py              # ğŸ  æœ¬åœ°è®­ç»ƒè„šæœ¬
â”œâ”€â”€ hf_jobs_train.py            # â˜ï¸ HF Jobsè®­ç»ƒè„šæœ¬
â”œâ”€â”€ pyproject.toml              # ğŸ“¦ uvä¾èµ–é…ç½®
â”œâ”€â”€ README.md                   # ğŸ“– é¡¹ç›®è¯´æ˜
â””â”€â”€ .gitignore                  # ğŸš« Gitå¿½ç•¥æ–‡ä»¶
```

## ğŸ‰ è·å–æˆæœ

è®­ç»ƒå®Œæˆåï¼Œä½ å°†è·å¾—ï¼š

1. **æœ¬åœ°æ¨¡å‹**: `./gemma3-tool-use/`
2. **Hubæ¨¡å‹**: `your-username/gemma3-1b-tool-use`
3. **è®­ç»ƒæ—¥å¿—**: Weights & Biasesè®°å½•

### ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# åŠ è½½å¾®è°ƒåçš„æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained("your-username/gemma3-1b-tool-use")
tokenizer = AutoTokenizer.from_pretrained("your-username/gemma3-1b-tool-use")

# è¿›è¡Œå·¥å…·è°ƒç”¨æ¨ç†
prompt = "What's the weather like in Beijing?"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=200)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

## ğŸš€ uvçš„ä¼˜åŠ¿

- **å¿«é€Ÿå®‰è£…**: æ¯”pipå¿«10-100å€
- **ä¾èµ–è§£æ**: æ›´æ™ºèƒ½çš„ä¾èµ–å†²çªè§£å†³
- **è™šæ‹Ÿç¯å¢ƒ**: è‡ªåŠ¨ç®¡ç†è™šæ‹Ÿç¯å¢ƒ
- **ç¼“å­˜ä¼˜åŒ–**: æ™ºèƒ½ç¼“å­˜å‡å°‘é‡å¤ä¸‹è½½
- **è·¨å¹³å°**: æ”¯æŒWindowsã€macOSã€Linux

## ğŸ“ è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ªMITè®¸å¯è¯ã€‚

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼
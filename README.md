# Gemma-3-1b Tool Use å¾®è°ƒé¡¹ç›®

åŸºäºç¬¬ä¸€æ€§åŸç†çš„Gemma-3-1bå·¥å…·è°ƒç”¨å¾®è°ƒé¡¹ç›®ï¼Œè®©æ¨¡å‹å­¦ä¼šä½¿ç”¨å·¥å…·ã€‚

## ğŸ¯ é¡¹ç›®ç›®æ ‡

- å¾®è°ƒGemma-3-1bæ¨¡å‹ä»¥æ”¯æŒå·¥å…·è°ƒç”¨
- ä½¿ç”¨çœŸå®çš„å·¥å…·è°ƒç”¨æ•°æ®é›†è¿›è¡Œè®­ç»ƒ
- åŸºäºLoRAæŠ€æœ¯è¿›è¡Œé«˜æ•ˆå¾®è°ƒ
- **ä½¿ç”¨uvè¿›è¡Œä¾èµ–ç®¡ç†**

## ğŸ“¦ ä¾èµ–ç®¡ç†

æœ¬é¡¹ç›®ä½¿ç”¨ `uv` è¿›è¡Œä¾èµ–ç®¡ç†ï¼š

```bash
# å®‰è£…ä¾èµ–
uv sync

# è¿è¡Œè®­ç»ƒè„šæœ¬
uv run python simple_train.py
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æœ¬åœ°è®­ç»ƒ

1. å…‹éš†é¡¹ç›®
```bash
git clone https://github.com/layue13/ft.git
cd ft
```

2. å®‰è£…ä¾èµ–
```bash
uv sync
```

3. è¿è¡Œè®­ç»ƒ
```bash
uv run python simple_train.py
```

### äº‘ç«¯è®­ç»ƒ (HF Jobs)

#### ç¡¬ä»¶é€‰æ‹©

æ ¹æ®[Hugging Face Jobsæ–‡æ¡£](https://huggingface.co/docs/huggingface_hub/main/en/guides/cli#hf-jobs)ï¼Œæœ‰å¤šç§ç¡¬ä»¶é€‰æ‹©ï¼š

**ç»æµå‹é€‰æ‹©** (æ¨è):
```bash
# T4 GPU - æ€§ä»·æ¯”æœ€é«˜
hf jobs run --flavor t4-small --secrets HF_TOKEN pytorch/pytorch:2.6.0-cuda12.4-cudnn9-devel bash -c "
apt-get update && apt-get install -y git &&
pip install uv &&
git clone https://github.com/layue13/ft.git &&
cd ft &&
uv sync &&
uv run python hf_jobs_train.py
"

# L4 GPU - ä¸­ç­‰æ€§èƒ½
hf jobs run --flavor l4x1 --secrets HF_TOKEN pytorch/pytorch:2.6.0-cuda12.4-cudnn9-devel bash -c "
apt-get update && apt-get install -y git &&
pip install uv &&
git clone https://github.com/layue13/ft.git &&
cd ft &&
uv sync &&
uv run python hf_jobs_train.py
"
```

**é«˜æ€§èƒ½é€‰æ‹©**:
```bash
# A10G GPU - å¹³è¡¡æ€§èƒ½
hf jobs run --flavor a10g-small --secrets HF_TOKEN pytorch/pytorch:2.6.0-cuda12.4-cudnn9-devel bash -c "
apt-get update && apt-get install -y git &&
pip install uv &&
git clone https://github.com/layue13/ft.git &&
cd ft &&
uv sync &&
uv run python hf_jobs_train.py
"

# A100 GPU - æœ€é«˜æ€§èƒ½
hf jobs run --flavor a100-large --secrets HF_TOKEN pytorch/pytorch:2.6.0-cuda12.4-cudnn9-devel bash -c "
apt-get update && apt-get install -y git &&
pip install uv &&
git clone https://github.com/layue13/ft.git &&
cd ft &&
uv sync &&
uv run python hf_jobs_train.py
"
```

**CPUé€‰æ‹©** (æœ€ç»æµ):
```bash
# CPUè®­ç»ƒ - æœ€ä¾¿å®œä½†è¾ƒæ…¢
hf jobs run --flavor cpu-upgrade --secrets HF_TOKEN pytorch/pytorch:2.6.0-cuda12.4-cudnn9-devel bash -c "
apt-get update && apt-get install -y git &&
pip install uv &&
git clone https://github.com/layue13/ft.git &&
cd ft &&
uv sync &&
uv run python hf_jobs_train.py
"
```

#### ç¡¬ä»¶é€‰æ‹©æŒ‡å—

| ç¡¬ä»¶ | é€‚ç”¨åœºæ™¯ | è®­ç»ƒæ—¶é—´ | æˆæœ¬ | æ¨èåº¦ |
|------|----------|----------|------|--------|
| **T4-small** | é¢„ç®—æœ‰é™ï¼Œä¸ç€æ€¥ | 2-4å°æ—¶ | $0.5-1 | â­â­â­â­â­ |
| **L4x1** | å¹³è¡¡æ€§èƒ½å’Œæˆæœ¬ | 1.5-3å°æ—¶ | $1-1.5 | â­â­â­â­ |
| **A10G-small** | å¿«é€Ÿè®­ç»ƒ | 1-2å°æ—¶ | $1-2 | â­â­â­ |
| **A100-large** | æœ€å¿«è®­ç»ƒ | 30-60åˆ†é’Ÿ | $2-4 | â­â­ |
| **CPU-upgrade** | æœ€ç»æµ | 4-8å°æ—¶ | $0.2-0.5 | â­â­â­ |

**æ¨è**: é¦–æ¬¡å°è¯•å»ºè®®ä½¿ç”¨ `t4-small`ï¼Œæ€§ä»·æ¯”æœ€é«˜ï¼

## ğŸ“Š é¡¹ç›®ç‰¹æ€§

| ç‰¹æ€§ | è¯´æ˜ |
|------|------|
| **æ¨¡å‹** | google/gemma-3-1b-it |
| **æ•°æ®é›†** | shawhin/tool-use-finetuning (477ä¸ªæ ·æœ¬) |
| **å¾®è°ƒæ–¹æ³•** | LoRA (Low-Rank Adaptation) |
| **ä¾èµ–ç®¡ç†** | uv |
| **è®­ç»ƒæ—¶é—´** | 2-4å°æ—¶ (T4) / 1-2å°æ—¶ (A10G) / 30-60åˆ†é’Ÿ (A100) |
| **é¢„æœŸæˆæœ¬** | $0.5-1 (T4) / $1-2 (A10G) / $2-4 (A100) |

## ğŸ”§ æŠ€æœ¯æ ˆ

- **æ¨¡å‹**: Gemma-3-1b-it
- **å¾®è°ƒæ–¹æ³•**: LoRA (r=16, alpha=32)
- **æ¡†æ¶**: Transformers + PEFT
- **æ•°æ®æ ¼å¼**: Gemmaå¯¹è¯æ ¼å¼
- **ä¼˜åŒ–å™¨**: AdamW (lr=2e-5)
- **ç²¾åº¦**: bfloat16 (GPU) / float32 (CPU)
- **ä¾èµ–ç®¡ç†**: uv

## ğŸ“ é¡¹ç›®ç»“æ„

```
ft/
â”œâ”€â”€ simple_train.py              # ğŸ  æœ¬åœ°è®­ç»ƒè„šæœ¬
â”œâ”€â”€ hf_jobs_train.py            # â˜ï¸ HF Jobsè®­ç»ƒè„šæœ¬
â”œâ”€â”€ merge_lora.py                # ğŸ”„ LoRAæƒé‡åˆå¹¶è„šæœ¬
â”œâ”€â”€ convert_to_gguf.py           # ğŸš€ GGUFæ ¼å¼è½¬æ¢è„šæœ¬
â”œâ”€â”€ pyproject.toml              # ğŸ“¦ uvä¾èµ–é…ç½®
â”œâ”€â”€ README.md                   # ğŸ“– é¡¹ç›®è¯´æ˜
â””â”€â”€ .gitignore                  # ğŸš« Gitå¿½ç•¥æ–‡ä»¶
```

## ğŸ‰ è·å–æˆæœ

è®­ç»ƒå®Œæˆåï¼Œä½ å°†è·å¾—ï¼š

1. **æœ¬åœ°æ¨¡å‹**: `./gemma3-tool-use/`
2. **Hubæ¨¡å‹**: `your-username/gemma3-1b-tool-use`
3. **è®­ç»ƒæ—¥å¿—**: Weights & Biasesè®°å½•

### ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹

#### æ–¹æ³•1: Pythonä»£ç ä½¿ç”¨

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# åŠ è½½å¾®è°ƒåçš„æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained("your-username/gemma3-1b-tool-use")
tokenizer = AutoTokenizer.from_pretrained("your-username/gemma3-1b-tool-use")

# è¿›è¡Œå·¥å…·è°ƒç”¨æ¨ç†
prompt = "What's the weather like in Beijing?"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=200)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

#### æ–¹æ³•2: LM Studioä½¿ç”¨ (æ¨è)

**æ­¥éª¤1: ä¸‹è½½æ¨¡å‹**
```bash
# ä»Hugging Face Hubä¸‹è½½æ¨¡å‹
git lfs install
git clone https://huggingface.co/your-username/gemma3-1b-tool-use
```

**æ­¥éª¤2: åœ¨LM Studioä¸­åŠ è½½**
1. æ‰“å¼€LM Studio
2. ç‚¹å‡» "Local Server" æ ‡ç­¾
3. ç‚¹å‡» "Browse" é€‰æ‹©æ¨¡å‹æ–‡ä»¶å¤¹ (`gemma3-1b-tool-use`)
4. ç‚¹å‡» "Load Model"

**æ­¥éª¤3: é…ç½®èŠå¤©ç•Œé¢**
1. åˆ‡æ¢åˆ° "Chat" æ ‡ç­¾
2. è®¾ç½®åˆé€‚çš„å‚æ•°ï¼š
   - **Temperature**: 0.7-0.9 (åˆ›é€ æ€§)
   - **Top P**: 0.9
   - **Max Tokens**: 512
   - **Stop Sequences**: `</s>`, `<eos>`

**æ­¥éª¤4: å·¥å…·è°ƒç”¨ç¤ºä¾‹**
```
ç”¨æˆ·: å¸®æˆ‘æŸ¥è¯¢åŒ—äº¬çš„å¤©æ°”

åŠ©æ‰‹: <tool_call>
{
 "tool_name": "weather",
 "args": {
   "location": "Beijing"
 }
}
</tool_call>
```

**æ­¥éª¤5: é«˜çº§é…ç½®**
- **Context Length**: 4096 (Gemma-3-1bæ”¯æŒ)
- **GPU Layers**: æ ¹æ®ä½ çš„GPUå†…å­˜è°ƒæ•´
- **Threads**: CPUæ ¸å¿ƒæ•°

#### æ–¹æ³•3: MLXä¼˜åŒ– (Apple Silicon)

å¯¹äºApple Silicon Macï¼Œå¯ä»¥ä½¿ç”¨MLXè·å¾—æœ€ä½³æ€§èƒ½ï¼š

```bash
# å®‰è£…MLX
pip install mlx

# ä½¿ç”¨MLXåŠ è½½æ¨¡å‹
import mlx.core as mx
from transformers import AutoTokenizer

# åŠ è½½tokenizer
tokenizer = AutoTokenizer.from_pretrained("your-username/gemma3-1b-tool-use")

# ä½¿ç”¨MLXè¿›è¡Œæ¨ç† (éœ€è¦MLXé€‚é…)
# æ³¨æ„: éœ€è¦å°†æ¨¡å‹è½¬æ¢ä¸ºMLXæ ¼å¼
```

#### æ–¹æ³•4: è½¬æ¢ä¸ºGGUFæ ¼å¼ (æ¨è)

è½¬æ¢ä¸ºGGUFæ ¼å¼è·å¾—æœ€ä½³æ€§èƒ½å’Œå…¼å®¹æ€§ï¼š

```bash
# æ–¹æ³•1: ä½¿ç”¨llama.cppè½¬æ¢
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make

# è½¬æ¢æ¨¡å‹ (éœ€è¦å…ˆåˆå¹¶LoRAæƒé‡)
python convert.py your-username/gemma3-1b-tool-use \
    --outfile gemma3-1b-tool-use.gguf \
    --outtype q4_k_m

# æ–¹æ³•2: ä½¿ç”¨transformers-to-gguf
pip install transformers-to-gguf
transformers-to-gguf your-username/gemma3-1b-tool-use \
    --output gemma3-1b-tool-use.gguf \
    --quantize q4_k_m
```

**GGUFä¼˜åŠ¿**:
- ğŸš€ **æ›´å¿«æ¨ç†**: æ¯”åŸæ ¼å¼å¿«2-5å€
- ğŸ’¾ **æ›´å°ä½“ç§¯**: é‡åŒ–åä½“ç§¯å‡å°‘50-75%
- ğŸ”§ **æ›´å¥½å…¼å®¹**: æ”¯æŒæ›´å¤šæ¨ç†æ¡†æ¶
- ğŸ–¥ï¸ **æ›´ä½èµ„æº**: å¯åœ¨CPUä¸Šé«˜æ•ˆè¿è¡Œ

#### æ–¹æ³•5: åˆå¹¶LoRAæƒé‡ (æ¨è)

ä¸ºäº†è·å¾—æœ€ä½³å…¼å®¹æ€§ï¼Œå»ºè®®å…ˆåˆå¹¶LoRAæƒé‡ï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# åŠ è½½åŸºç¡€æ¨¡å‹
base_model = AutoModelForCausalLM.from_pretrained("google/gemma-3-1b-it")
tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-1b-it")

# åŠ è½½LoRAæƒé‡
model = PeftModel.from_pretrained(base_model, "your-username/gemma3-1b-tool-use")

# åˆå¹¶æƒé‡
merged_model = model.merge_and_unload()

# ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
merged_model.save_pretrained("./gemma3-1b-tool-use-merged")
tokenizer.save_pretrained("./gemma3-1b-tool-use-merged")

print("âœ… LoRAæƒé‡å·²åˆå¹¶ï¼Œæ¨¡å‹å·²ä¿å­˜")
```

**åˆå¹¶åçš„ä¼˜åŠ¿**:
- âœ… **å®Œå…¨å…¼å®¹**: æ‰€æœ‰æ¨ç†æ¡†æ¶éƒ½æ”¯æŒ
- ğŸš€ **æ›´å¿«åŠ è½½**: æ— éœ€åŠ¨æ€åŠ è½½LoRA
- ğŸ’¾ **æ›´å°ä½“ç§¯**: æ¯”åˆ†ç¦»å­˜å‚¨æ›´ç´§å‡‘
- ğŸ”§ **æ›´å¥½éƒ¨ç½²**: é€‚åˆç”Ÿäº§ç¯å¢ƒ

### ğŸš€ æ‰‹åŠ¨è½¬æ¢è„šæœ¬

ä½¿ç”¨ä¸¤ä¸ªç‹¬ç«‹çš„è„šæœ¬è¿›è¡Œæ‰‹åŠ¨è½¬æ¢ï¼š

#### æ­¥éª¤1: åˆå¹¶LoRAæƒé‡

```bash
# è¿è¡ŒLoRAæƒé‡åˆå¹¶è„šæœ¬
python merge_lora.py

# æŒ‰æç¤ºè¾“å…¥:
# - æ¨¡å‹åç§° (ä¾‹å¦‚: layue13/gemma3-1b-tool-use)
# - è¾“å‡ºç›®å½• (é»˜è®¤: ./gemma3-1b-tool-use-merged)
```

#### æ­¥éª¤2: è½¬æ¢ä¸ºGGUFæ ¼å¼

```bash
# è¿è¡ŒGGUFè½¬æ¢è„šæœ¬
python convert_to_gguf.py

# æŒ‰æç¤ºè¾“å…¥:
# - æ¨¡å‹è·¯å¾„ (åˆå¹¶åçš„æ¨¡å‹ç›®å½•)
# - è¾“å‡ºæ–‡ä»¶ (é»˜è®¤: ./gemma3-1b-tool-use.gguf)
# - é‡åŒ–ç±»å‹ (æ¨è: q4_k_m)
```

**è½¬æ¢è„šæœ¬åŠŸèƒ½**:
- ğŸ”„ **ç‹¬ç«‹åˆå¹¶**: ä¸“é—¨çš„LoRAæƒé‡åˆå¹¶
- ğŸš€ **GGUFè½¬æ¢**: æ”¯æŒå¤šç§é‡åŒ–é€‰é¡¹
- ğŸ“ **è‡ªåŠ¨ç”Ÿæˆ**: ç”Ÿæˆä½¿ç”¨ç¤ºä¾‹è„šæœ¬
- ğŸ”§ **ä¾èµ–æ£€æŸ¥**: è‡ªåŠ¨æ£€æŸ¥å’Œå®‰è£…ä¾èµ–

### å·¥å…·è°ƒç”¨æ ¼å¼è¯´æ˜

è®­ç»ƒåçš„æ¨¡å‹æ”¯æŒä»¥ä¸‹æ ¼å¼ï¼š

```
<bos><start_of_turn>user
ä½ çš„é—®é¢˜
<end_of_turn>
<start_of_turn>model
<tool_call>
{
 "tool_name": "å·¥å…·åç§°",
 "args": {
   "å‚æ•°1": "å€¼1",
   "å‚æ•°2": "å€¼2"
 }
}
</tool_call>
<end_of_turn><eos>
```

## ğŸš€ uvçš„ä¼˜åŠ¿

- **å¿«é€Ÿå®‰è£…**: æ¯”pipå¿«10-100å€
- **ä¾èµ–è§£æ**: æ›´æ™ºèƒ½çš„ä¾èµ–å†²çªè§£å†³
- **è™šæ‹Ÿç¯å¢ƒ**: è‡ªåŠ¨ç®¡ç†è™šæ‹Ÿç¯å¢ƒ
- **ç¼“å­˜ä¼˜åŒ–**: æ™ºèƒ½ç¼“å­˜å‡å°‘é‡å¤ä¸‹è½½
- **è·¨å¹³å°**: æ”¯æŒWindowsã€macOSã€Linux

## ğŸ“ è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ªMITè®¸å¯è¯ã€‚

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼
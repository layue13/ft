#!/usr/bin/env python3
"""
æ¨¡å‹è½¬æ¢è„šæœ¬ - æ”¯æŒLoRAæƒé‡åˆå¹¶å’ŒGGUFè½¬æ¢
åŸºäºç¬¬ä¸€æ€§åŸç†ï¼šå…¼å®¹æ€§ + æ€§èƒ½ + æ˜“ç”¨æ€§
"""

import os
import subprocess
import sys
from pathlib import Path

def install_dependencies():
    """å®‰è£…å¿…è¦çš„ä¾èµ–"""
    print("ğŸ“¦ å®‰è£…è½¬æ¢ä¾èµ–...")
    
    dependencies = [
        "transformers",
        "peft", 
        "torch",
        "transformers-to-gguf"
    ]
    
    for dep in dependencies:
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", dep])
            print(f"âœ… å·²å®‰è£… {dep}")
        except subprocess.CalledProcessError:
            print(f"âš ï¸ å®‰è£… {dep} å¤±è´¥")

def merge_lora_weights(model_name, output_dir):
    """åˆå¹¶LoRAæƒé‡"""
    print(f"ğŸ”§ åˆå¹¶LoRAæƒé‡: {model_name}")
    
    merge_script = f"""
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

print("ğŸ“¦ åŠ è½½åŸºç¡€æ¨¡å‹...")
base_model = AutoModelForCausalLM.from_pretrained("google/gemma-3-1b-it")
tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-1b-it")

print("ğŸ“¦ åŠ è½½LoRAæƒé‡...")
model = PeftModel.from_pretrained(base_model, "{model_name}")

print("ğŸ”§ åˆå¹¶æƒé‡...")
merged_model = model.merge_and_unload()

print("ğŸ’¾ ä¿å­˜åˆå¹¶åçš„æ¨¡å‹...")
merged_model.save_pretrained("{output_dir}")
tokenizer.save_pretrained("{output_dir}")

print("âœ… LoRAæƒé‡åˆå¹¶å®Œæˆï¼")
"""
    
    with open("temp_merge.py", "w") as f:
        f.write(merge_script)
    
    try:
        subprocess.check_call([sys.executable, "temp_merge.py"])
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}")
    except subprocess.CalledProcessError as e:
        print(f"âŒ åˆå¹¶å¤±è´¥: {e}")
        return False
    finally:
        if os.path.exists("temp_merge.py"):
            os.remove("temp_merge.py")
    
    return True

def convert_to_gguf(model_path, output_file):
    """è½¬æ¢ä¸ºGGUFæ ¼å¼"""
    print(f"ğŸ”„ è½¬æ¢ä¸ºGGUF: {model_path} -> {output_file}")
    
    try:
        # ä½¿ç”¨transformers-to-gguf
        cmd = [
            "transformers-to-gguf",
            model_path,
            "--output", output_file,
            "--quantize", "q4_k_m"
        ]
        
        subprocess.check_call(cmd)
        print(f"âœ… GGUFè½¬æ¢å®Œæˆ: {output_file}")
        return True
        
    except subprocess.CalledProcessError as e:
        print(f"âŒ GGUFè½¬æ¢å¤±è´¥: {e}")
        return False

def create_mlx_script(model_path):
    """åˆ›å»ºMLXä½¿ç”¨è„šæœ¬"""
    print("ğŸ“ åˆ›å»ºMLXä½¿ç”¨è„šæœ¬...")
    
    mlx_script = f"""
#!/usr/bin/env python3
\"\"\"
MLXæ¨ç†è„šæœ¬ - ç”¨äºApple Siliconä¼˜åŒ–
\"\"\"

import mlx.core as mx
from transformers import AutoTokenizer
import json

# åŠ è½½tokenizer
tokenizer = AutoTokenizer.from_pretrained("{model_path}")

def generate_tool_call(prompt, max_length=512):
    \"\"\"ç”Ÿæˆå·¥å…·è°ƒç”¨å“åº”\"\"\"
    # æ ¼å¼åŒ–è¾“å…¥
    formatted_prompt = f"<bos><start_of_turn>user\\n{{prompt}}<end_of_turn>\\n<start_of_turn>model\\n"
    
    # è¿™é‡Œéœ€è¦MLXæ¨¡å‹åŠ è½½å’Œæ¨ç†
    # æ³¨æ„: éœ€è¦å…ˆå°†æ¨¡å‹è½¬æ¢ä¸ºMLXæ ¼å¼
    print("âš ï¸ éœ€è¦å…ˆå°†æ¨¡å‹è½¬æ¢ä¸ºMLXæ ¼å¼")
    print("ğŸ“ ä½¿ç”¨è½¬æ¢åçš„æ¨¡å‹è¿›è¡Œæ¨ç†")
    
    return "å·¥å…·è°ƒç”¨å“åº”"

# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    prompt = "å¸®æˆ‘æŸ¥è¯¢åŒ—äº¬çš„å¤©æ°”"
    response = generate_tool_call(prompt)
    print(f"ç”¨æˆ·: {{prompt}}")
    print(f"åŠ©æ‰‹: {{response}}")
"""
    
    with open("mlx_inference.py", "w") as f:
        f.write(mlx_script)
    
    print("âœ… MLXè„šæœ¬å·²åˆ›å»º: mlx_inference.py")

def main():
    print("ğŸš€ å¼€å§‹æ¨¡å‹è½¬æ¢...")
    
    # é…ç½®
    model_name = input("è¯·è¾“å…¥æ¨¡å‹åç§° (ä¾‹å¦‚: layue13/gemma3-1b-tool-use): ").strip()
    if not model_name:
        model_name = "layue13/gemma3-1b-tool-use"
    
    output_dir = "./gemma3-1b-tool-use-merged"
    gguf_file = "./gemma3-1b-tool-use.gguf"
    
    # 1. å®‰è£…ä¾èµ–
    install_dependencies()
    
    # 2. åˆå¹¶LoRAæƒé‡
    print("\n" + "="*50)
    if merge_lora_weights(model_name, output_dir):
        print("âœ… LoRAæƒé‡åˆå¹¶æˆåŠŸ")
    else:
        print("âŒ LoRAæƒé‡åˆå¹¶å¤±è´¥")
        return
    
    # 3. è½¬æ¢ä¸ºGGUF
    print("\n" + "="*50)
    if convert_to_gguf(output_dir, gguf_file):
        print("âœ… GGUFè½¬æ¢æˆåŠŸ")
    else:
        print("âŒ GGUFè½¬æ¢å¤±è´¥")
    
    # 4. åˆ›å»ºMLXè„šæœ¬
    print("\n" + "="*50)
    create_mlx_script(output_dir)
    
    # 5. ä½¿ç”¨è¯´æ˜
    print("\n" + "="*50)
    print("ğŸ‰ è½¬æ¢å®Œæˆï¼")
    print("\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print(f"  - åˆå¹¶æ¨¡å‹: {output_dir}")
    print(f"  - GGUFæ–‡ä»¶: {gguf_file}")
    print(f"  - MLXè„šæœ¬: mlx_inference.py")
    
    print("\nğŸ”§ ä½¿ç”¨æ–¹æ³•:")
    print("1. åˆå¹¶æ¨¡å‹: ç›´æ¥ä½¿ç”¨transformersåŠ è½½")
    print("2. GGUFæ–‡ä»¶: ä½¿ç”¨llama.cppæˆ–LM StudioåŠ è½½")
    print("3. MLXè„šæœ¬: åœ¨Apple Silicon Macä¸Šè¿è¡Œ")
    
    print("\nğŸ’¡ æ¨è:")
    print("- æœ¬åœ°æ¨ç†: ä½¿ç”¨GGUFæ ¼å¼")
    print("- Apple Silicon: ä½¿ç”¨MLXä¼˜åŒ–")
    print("- ç”Ÿäº§éƒ¨ç½²: ä½¿ç”¨åˆå¹¶åçš„æ¨¡å‹")

if __name__ == "__main__":
    main()

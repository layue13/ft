#!/usr/bin/env python3
"""
Gemma-3-1b Tool Use å¾®è°ƒè„šæœ¬
åŸºäºç¬¬ä¸€æ€§åŸç†ï¼šæ¨¡å‹ + æ•°æ® + è®­ç»ƒå¾ªç¯
ç›®æ ‡ï¼šè®©Gemma-3-1bæ”¯æŒå·¥å…·è°ƒç”¨
"""

import torch
from transformers import (
    AutoModelForCausalLM, AutoTokenizer, 
    TrainingArguments, Trainer
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import load_dataset

def main():
    print("ğŸš€ å¼€å§‹Gemma-3-1b Tool Useå¾®è°ƒ...")
    
    # 1. æ¨¡å‹å’Œåˆ†è¯å™¨ - Gemma-3-1b-it
    model_name = "google/gemma-3-1b-it"  # ä½¿ç”¨æ­£ç¡®çš„Gemma-3-1bæ¨¡å‹
    tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side="right")  # Gemmaä½¿ç”¨right padding
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    
    # 2. LoRAé…ç½® - é’ˆå¯¹Gemmaæ¨¡å‹
    lora_config = LoraConfig(
        r=16, lora_alpha=32, lora_dropout=0.1,  # å¢å¤§rankä»¥æé«˜å·¥å…·è°ƒç”¨èƒ½åŠ›
        task_type=TaskType.CAUSAL_LM,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]  # Gemmaç‰¹å®šæ¨¡å—
    )
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # 3. Tool Useæ•°æ® - shawhin/tool-use-finetuningæ•°æ®é›†
    dataset = load_dataset("shawhin/tool-use-finetuning", split="train[:200]")  # å¢åŠ æ ·æœ¬é‡
    
    def format_tool_use_data(example):
        """æ ¼å¼åŒ–å·¥å…·è°ƒç”¨æ•°æ®ä¸ºGemmaå¯¹è¯æ ¼å¼"""
        if "trace" not in example or not example.get("tool_needed"):
            return {"text": "<bos><start_of_turn>user\nhello<end_of_turn>\n<start_of_turn>model\nHello! How can I help you?<end_of_turn><eos>"}
        
        conversation = "<bos>"
        
        # å¤„ç†å¯¹è¯å†å²
        for msg in example["trace"]:
            role = msg.get("role", "")
            content = msg.get("content", "")
            
            if role == "user":
                conversation += f"<start_of_turn>user\n{content}<end_of_turn>\n"
        
        # æ·»åŠ å·¥å…·è°ƒç”¨å“åº”
        if example.get("tool_needed") and example.get("tool_name"):
            tool_call = f'<tool_call>\n{{\n "tool_name": "{example["tool_name"]}",\n "args": {{}}\n}}\n</tool_call>'
            conversation += f"<start_of_turn>model\n{tool_call}<end_of_turn>"
        
        conversation += "<eos>"
        return {"text": conversation}
    
    dataset = dataset.map(format_tool_use_data)
    
    def tokenize(examples):
        return tokenizer(
            examples["text"], 
            truncation=True, 
            padding="max_length",
            max_length=512,  # å·¥å…·è°ƒç”¨éœ€è¦æ›´é•¿åºåˆ—
            return_tensors="pt"
        )
    
    tokenized = dataset.map(tokenize, batched=True, remove_columns=dataset.column_names)
    tokenized = tokenized.add_column("labels", tokenized["input_ids"])
    
    # 4. è®­ç»ƒå‚æ•° - Tool Useä¼˜åŒ–
    training_args = TrainingArguments(
        output_dir="./gemma3-tool-use",
        num_train_epochs=2,  # å·¥å…·è°ƒç”¨éœ€è¦æ›´å¤šè®­ç»ƒ
        per_device_train_batch_size=1,  # Gemmaæ¨¡å‹è¾ƒå¤§ï¼Œå‡å°batch size
        gradient_accumulation_steps=4,  # é€šè¿‡æ¢¯åº¦ç´¯ç§¯å¢åŠ æœ‰æ•ˆbatch size
        learning_rate=2e-5,  # é™ä½å­¦ä¹ ç‡ï¼Œé¿å…ç ´åé¢„è®­ç»ƒçŸ¥è¯†
        warmup_ratio=0.1,
        logging_steps=10,
        save_strategy="epoch",
        save_total_limit=2,
        push_to_hub=True,
        hub_model_id="gemma3-1b-tool-use",  # æŒ‡å®šHubæ¨¡å‹å
        report_to="none",
        remove_unused_columns=False,
        dataloader_num_workers=0,
        bf16=True,  # ä½¿ç”¨bf16æé«˜æ•ˆç‡
        gradient_checkpointing=True,  # èŠ‚çœæ˜¾å­˜
    )
    
    # 5. è®­ç»ƒå™¨ - Tool Useä¸“ç”¨
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized,
        tokenizer=tokenizer
    )
    
    # 6. å¼€å§‹è®­ç»ƒ
    print("âœ¨ å¼€å§‹Tool Useå¾®è°ƒ...")
    print(f"ğŸ“Š è®­ç»ƒæ ·æœ¬: {len(tokenized)}")
    print(f"ğŸ¯ ç›®æ ‡: è®©Gemma-3-1bå­¦ä¼šå·¥å…·è°ƒç”¨")
    
    trainer.train()
    
    # 7. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    trainer.save_model()
    tokenizer.save_pretrained("./gemma3-tool-use")
    
    print("ğŸ‰ Gemma-3-1b Tool Useå¾®è°ƒå®Œæˆï¼")
    print("ğŸ“¤ æ¨¡å‹å·²æ¨é€åˆ°Hugging Face Hub")
    print("ğŸ›  ç°åœ¨å¯ä»¥ä½¿ç”¨å·¥å…·è°ƒç”¨åŠŸèƒ½äº†ï¼")

if __name__ == "__main__":
    main()
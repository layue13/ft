#!/usr/bin/env python3
"""
æ‰‹åŠ¨è½¬æ¢è„šæœ¬ - å°†åˆå¹¶åçš„æ¨¡å‹è½¬æ¢ä¸ºGGUFæ ¼å¼
"""

import os
import subprocess
import sys

def main():
    print("ğŸš€ å¼€å§‹æ‰‹åŠ¨è½¬æ¢...")
    
    model_path = "./gemma3-1b-tool-use-merged"
    output_file = "./gemma3-1b-tool-use.gguf"
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹ä¸å­˜åœ¨: {model_path}")
        return
    
    print(f"ğŸ“¦ æºæ¨¡å‹: {model_path}")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
    
    # æ–¹æ³•1: å°è¯•ä½¿ç”¨llama.cppçš„è½¬æ¢è„šæœ¬
    print("\nğŸ”„ æ–¹æ³•1: ä½¿ç”¨llama.cppè½¬æ¢...")
    try:
        # ä½¿ç”¨ç³»ç»ŸPythonè¿è¡Œllama.cppè½¬æ¢
        cmd = [
            sys.executable, 
            "llama.cpp/convert_hf_to_gguf.py",
            model_path,
            "--outfile", output_file,
            "--outtype", "q4_k_m"
        ]
        
        print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if result.returncode == 0:
            print("âœ… è½¬æ¢æˆåŠŸï¼")
            if os.path.exists(output_file):
                file_size = os.path.getsize(output_file) / (1024 * 1024)  # MB
                print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
                print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
            return
        else:
            print(f"âŒ è½¬æ¢å¤±è´¥: {result.stderr}")
    except Exception as e:
        print(f"âŒ æ–¹æ³•1å¤±è´¥: {e}")
    
    # æ–¹æ³•2: ä½¿ç”¨transformersä¿å­˜ä¸ºä¸´æ—¶æ ¼å¼
    print("\nğŸ”„ æ–¹æ³•2: ä½¿ç”¨transformersä¿å­˜...")
    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        print("ğŸ“¦ åŠ è½½æ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained(model_path)
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        temp_dir = "./temp_model_for_gguf"
        print(f"ğŸ’¾ ä¿å­˜åˆ°ä¸´æ—¶ç›®å½•: {temp_dir}")
        
        model.save_pretrained(temp_dir)
        tokenizer.save_pretrained(temp_dir)
        
        print("âœ… æ¨¡å‹å·²ä¿å­˜åˆ°ä¸´æ—¶ç›®å½•")
        print(f"ğŸ“ ä¸´æ—¶ç›®å½•: {temp_dir}")
        
        # æä¾›æ‰‹åŠ¨è½¬æ¢æŒ‡å¯¼
        print("\nğŸ’¡ æ‰‹åŠ¨è½¬æ¢æ­¥éª¤:")
        print("1. å®‰è£…llama.cppä¾èµ–:")
        print("   pip install torch transformers sentencepiece")
        print("2. è¿è¡Œè½¬æ¢:")
        print(f"   python llama.cpp/convert_hf_to_gguf.py {temp_dir} --outfile {output_file} --outtype q4_k_m")
        
    except Exception as e:
        print(f"âŒ æ–¹æ³•2å¤±è´¥: {e}")
    
    # æ–¹æ³•3: æä¾›å…¶ä»–è½¬æ¢é€‰é¡¹
    print("\nğŸ”„ æ–¹æ³•3: å…¶ä»–è½¬æ¢é€‰é¡¹...")
    print("ğŸ’¡ å¯ä»¥å°è¯•ä»¥ä¸‹æ–¹æ³•:")
    print("1. ä½¿ç”¨LM Studioç›´æ¥åŠ è½½åˆå¹¶åçš„æ¨¡å‹")
    print("2. ä½¿ç”¨Ollamaè½¬æ¢")
    print("3. ä½¿ç”¨å…¶ä»–GGUFè½¬æ¢å·¥å…·")

if __name__ == "__main__":
    main()

#!/usr/bin/env python3
"""
Gemma3-1b å·¥å…·è°ƒç”¨å¾®è°ƒ - Hugging Face Spaceç‰ˆæœ¬
"""

import os
import sys
import logging
import torch
import gradio as gr
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent / "src"))

from src.utils import setup_logging, load_config, validate_config, create_output_dir
from src.model_config import load_model_and_tokenizer
from src.data_processor import create_data_processor
from src.trainer import create_trainer


def setup_environment():
    """è®¾ç½®ç¯å¢ƒ"""
    setup_logging("INFO")
    logger = logging.getLogger(__name__)
    
    # æ£€æŸ¥CUDA
    logger.info(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        logger.info(f"GPUè®¾å¤‡: {torch.cuda.get_device_name()}")
        logger.info(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    return logger


def start_training(config_path: str, output_dir: str = "./outputs", max_samples: int = 100):
    """å¼€å§‹è®­ç»ƒ"""
    logger = logging.getLogger(__name__)
    
    try:
        # åŠ è½½é…ç½®
        logger.info(f"åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
        config = load_config(config_path)
        
        # éªŒè¯é…ç½®
        validate_config(config)
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        config["training"]["output_dir"] = output_dir
        create_output_dir(output_dir)
        
        # é™åˆ¶è®­ç»ƒæ ·æœ¬æ•°
        config["dataset"]["max_samples"] = max_samples
        
        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        logger.info("åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
        model, tokenizer = load_model_and_tokenizer(config)
        
        # åˆ›å»ºæ•°æ®å¤„ç†å™¨
        logger.info("åˆ›å»ºæ•°æ®å¤„ç†å™¨...")
        data_processor = create_data_processor(tokenizer, config)
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        logger.info("å‡†å¤‡è®­ç»ƒæ•°æ®...")
        dataset_name = config["dataset"]["name"]
        datasets = data_processor.prepare_training_data(dataset_name)
        
        train_dataset = datasets["train"]
        eval_dataset = datasets["eval"]
        
        logger.info(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
        logger.info(f"éªŒè¯é›†å¤§å°: {len(eval_dataset)}")
        
        # åˆ›å»ºè®­ç»ƒå™¨
        logger.info("åˆ›å»ºè®­ç»ƒå™¨...")
        trainer = create_trainer(model, tokenizer, config)
        
        # å¼€å§‹è®­ç»ƒ
        logger.info("å¼€å§‹è®­ç»ƒ...")
        trainer.train(train_dataset, eval_dataset)
        
        # æœ€ç»ˆè¯„ä¼°
        logger.info("è¿›è¡Œæœ€ç»ˆè¯„ä¼°...")
        results = trainer.evaluate(eval_dataset)
        
        logger.info("è®­ç»ƒå®Œæˆ!")
        logger.info(f"æœ€ç»ˆè¯„ä¼°ç»“æœ: {results}")
        
        return f"âœ… è®­ç»ƒå®Œæˆ!\n\nğŸ“Š è¯„ä¼°ç»“æœ:\n{results}\n\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}"
        
    except Exception as e:
        error_msg = f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}"
        logger.error(error_msg)
        return error_msg


def create_interface():
    """åˆ›å»ºGradioç•Œé¢"""
    
    with gr.Blocks(title="Gemma3-1b å·¥å…·è°ƒç”¨å¾®è°ƒ", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸš€ Gemma3-1b å·¥å…·è°ƒç”¨å¾®è°ƒ")
        gr.Markdown("ä½¿ç”¨PEFTå¾®è°ƒGemma3-1bæ¨¡å‹ä»¥æ”¯æŒå·¥å…·è°ƒç”¨åŠŸèƒ½")
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("## âš™ï¸ é…ç½®")
                
                with gr.Group():
                    config_path = gr.Textbox(
                        label="é…ç½®æ–‡ä»¶è·¯å¾„",
                        value="configs/space_config.yaml",
                        placeholder="é…ç½®æ–‡ä»¶è·¯å¾„",
                        info="è®­ç»ƒé…ç½®æ–‡ä»¶è·¯å¾„ (æ¨èä½¿ç”¨space_config.yaml)"
                    )
                    
                    output_dir = gr.Textbox(
                        label="è¾“å‡ºç›®å½•",
                        value="./outputs",
                        placeholder="æ¨¡å‹è¾“å‡ºç›®å½•",
                        info="è®­ç»ƒç»“æœä¿å­˜ç›®å½•"
                    )
                    
                    max_samples = gr.Slider(
                        minimum=10,
                        maximum=1000,
                        value=100,
                        step=10,
                        label="æœ€å¤§è®­ç»ƒæ ·æœ¬æ•°",
                        info="é™åˆ¶è®­ç»ƒæ ·æœ¬æ•°é‡ä»¥èŠ‚çœæ—¶é—´"
                    )
                
                with gr.Group():
                    start_btn = gr.Button("ğŸš€ å¼€å§‹è®­ç»ƒ", variant="primary", size="lg")
                    stop_btn = gr.Button("â¹ï¸ åœæ­¢è®­ç»ƒ", variant="stop", size="lg")
            
            with gr.Column(scale=2):
                gr.Markdown("## ğŸ“Š è®­ç»ƒçŠ¶æ€")
                
                with gr.Group():
                    status_output = gr.Textbox(
                        label="è®­ç»ƒæ—¥å¿—",
                        lines=25,
                        max_lines=100,
                        interactive=False,
                        show_copy_button=True
                    )
                    
                    progress_bar = gr.Progress()
        
        # ç»‘å®šäº‹ä»¶
        start_btn.click(
            fn=lambda: "å¼€å§‹è®­ç»ƒ...\nè¯·ç­‰å¾…æ¨¡å‹åŠ è½½...",
            outputs=status_output
        ).then(
            fn=start_training,
            inputs=[config_path, output_dir, max_samples],
            outputs=status_output,
            show_progress=True
        )
        
        stop_btn.click(
            fn=lambda: "è®­ç»ƒå·²åœæ­¢",
            outputs=status_output
        )
        
        gr.Markdown("""
        ## ğŸ“– ä½¿ç”¨è¯´æ˜
        
        1. **å‡†å¤‡é˜¶æ®µ**: ç¡®ä¿é…ç½®æ–‡ä»¶å­˜åœ¨ï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨ä¸‹è½½
        2. **è®­ç»ƒé˜¶æ®µ**: ç‚¹å‡»"å¼€å§‹è®­ç»ƒ"æŒ‰é’®ï¼Œç›‘æ§å³ä¾§æ—¥å¿—
        3. **å®Œæˆé˜¶æ®µ**: è®­ç»ƒå®Œæˆåæ¨¡å‹ä¼šä¿å­˜åˆ°æŒ‡å®šç›®å½•
        
        ## ğŸ–¥ï¸ ç¡¬ä»¶ä¿¡æ¯
        
        - **GPU**: å½“å‰Spaceé…ç½®çš„GPU
        - **å†…å­˜**: è‡ªåŠ¨åˆ†é…
        - **å­˜å‚¨**: 50GBå¯ç”¨ç©ºé—´
        
        ## ğŸ’¡ ä¼˜åŒ–å»ºè®®
        
        - è°ƒæ•´"æœ€å¤§è®­ç»ƒæ ·æœ¬æ•°"ä»¥æ§åˆ¶è®­ç»ƒæ—¶é—´
        - ä½¿ç”¨è¾ƒå°çš„batch sizeä»¥èŠ‚çœæ˜¾å­˜
        - ç›‘æ§è®­ç»ƒæ—¥å¿—ä»¥äº†è§£è¿›åº¦
        
        ## ğŸ†˜ å¸¸è§é—®é¢˜
        
        - **æ¨¡å‹ä¸‹è½½å¤±è´¥**: æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒHF Token
        - **æ˜¾å­˜ä¸è¶³**: å‡å°‘batch sizeæˆ–è®­ç»ƒæ ·æœ¬æ•°
        - **è®­ç»ƒä¸­æ–­**: æ£€æŸ¥Spaceæ—¥å¿—è·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯
        
        ## ğŸ”— ç›¸å…³é“¾æ¥
        
        - [é¡¹ç›®æ–‡æ¡£](https://github.com/your-repo)
        - [æ•°æ®é›†ä¿¡æ¯](https://huggingface.co/datasets/shawhin/tool-use-finetuning)
        - [Gemma3æ¨¡å‹](https://huggingface.co/google/gemma-3-1b-it)
        """)
    
    return demo


if __name__ == "__main__":
    # è®¾ç½®ç¯å¢ƒ
    logger = setup_environment()
    
    # åˆ›å»ºç•Œé¢
    demo = create_interface()
    
    # å¯åŠ¨åº”ç”¨
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=True
    )

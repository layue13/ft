#!/usr/bin/env python3
"""
LoRAæƒé‡åˆå¹¶è„šæœ¬
å°†LoRAæƒé‡åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹ä¸­ï¼Œè·å¾—å®Œå…¨å…¼å®¹çš„æ¨¡å‹
"""

import os
import sys
from pathlib import Path

def merge_lora_weights(model_name, output_dir):
    """åˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹"""
    print(f"ğŸ”§ å¼€å§‹åˆå¹¶LoRAæƒé‡...")
    print(f"ğŸ“¦ æºæ¨¡å‹: {model_name}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer
        from peft import PeftModel
        
        print("ğŸ“¦ åŠ è½½åŸºç¡€æ¨¡å‹ (google/gemma-3-1b-it)...")
        base_model = AutoModelForCausalLM.from_pretrained(
            "google/gemma-3-1b-it",
            torch_dtype="auto",
            device_map="auto"
        )
        tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-1b-it")
        
        print("ğŸ“¦ åŠ è½½LoRAæƒé‡...")
        model = PeftModel.from_pretrained(base_model, model_name)
        
        print("ğŸ”§ åˆå¹¶æƒé‡...")
        merged_model = model.merge_and_unload()
        
        print("ğŸ’¾ ä¿å­˜åˆå¹¶åçš„æ¨¡å‹...")
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        merged_model.save_pretrained(output_dir)
        tokenizer.save_pretrained(output_dir)
        
        print(f"âœ… LoRAæƒé‡åˆå¹¶å®Œæˆï¼")
        print(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}")
        
        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in merged_model.parameters())
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°æ€»æ•°: {total_params:,}")
        
        return True
        
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
        print("è¯·è¿è¡Œ: pip install transformers peft torch")
        return False
    except Exception as e:
        print(f"âŒ åˆå¹¶å¤±è´¥: {e}")
        return False

def main():
    print("ğŸš€ LoRAæƒé‡åˆå¹¶å·¥å…·")
    print("=" * 50)
    
    # è·å–è¾“å…¥
    model_name = input("è¯·è¾“å…¥æ¨¡å‹åç§° (ä¾‹å¦‚: layue13/gemma3-1b-tool-use): ").strip()
    if not model_name:
        print("âŒ æ¨¡å‹åç§°ä¸èƒ½ä¸ºç©º")
        return
    
    output_dir = input("è¯·è¾“å…¥è¾“å‡ºç›®å½• (é»˜è®¤: ./gemma3-1b-tool-use-merged): ").strip()
    if not output_dir:
        output_dir = "./gemma3-1b-tool-use-merged"
    
    print("\n" + "=" * 50)
    
    # æ‰§è¡Œåˆå¹¶
    if merge_lora_weights(model_name, output_dir):
        print("\nğŸ‰ åˆå¹¶æˆåŠŸï¼")
        print(f"ğŸ“ åˆå¹¶åçš„æ¨¡å‹: {output_dir}")
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print("1. ä½¿ç”¨åˆå¹¶åçš„æ¨¡å‹è¿›è¡Œæ¨ç†")
        print("2. è¿è¡Œ convert_to_gguf.py è½¬æ¢ä¸ºGGUFæ ¼å¼")
        print("3. åœ¨LM Studioä¸­åŠ è½½ä½¿ç”¨")
    else:
        print("\nâŒ åˆå¹¶å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")

if __name__ == "__main__":
    main()

# Hugging Face Jobs äº‘ç«¯å¾®è°ƒæŒ‡å—

## ğŸš€ ä»€ä¹ˆæ˜¯HF Jobsï¼Ÿ

HF Jobsæ˜¯Hugging Faceçš„äº‘ç«¯è®¡ç®—å¹³å°ï¼Œè®©ä½ å¯ä»¥åœ¨å¼ºå¤§çš„GPUä¸Šè®­ç»ƒæ¨¡å‹è€Œæ— éœ€æœ¬åœ°ç¡¬ä»¶ã€‚

## ğŸ“‹ å‰ç½®æ¡ä»¶

1. **Proè´¦æˆ·**ï¼šéœ€è¦HF Proè®¢é˜…ï¼ˆ$9/æœˆï¼‰
2. **CLIå·¥å…·**ï¼šæœ€æ–°ç‰ˆæœ¬çš„huggingface-cli

## ğŸ› ï¸ å¿«é€Ÿå¼€å§‹

### 1. è®¾ç½®ç¯å¢ƒ

```bash
# å‡çº§huggingface-hub (å·²å®Œæˆ)
uv add "huggingface_hub[cli]"

# ç™»å½•ä½ çš„Proè´¦æˆ·
uv run hf auth login

# æ£€æŸ¥ç™»å½•çŠ¶æ€
uv run hf auth whoami

# æŸ¥çœ‹å¯ç”¨ä»»åŠ¡
uv run hf jobs ps
```

### 2. æŸ¥çœ‹å¯ç”¨ç¡¬ä»¶

```bash
# CPUé€‰é¡¹
- cpu-basic: åŸºç¡€CPU
- cpu-upgrade: é«˜æ€§èƒ½CPU

# GPUé€‰é¡¹  
- t4-small: NVIDIA T4 (16GB)
- a10g-small: NVIDIA A10G (24GB)
- a100-large: NVIDIA A100 (80GB)
```

### 3. æäº¤è®­ç»ƒä»»åŠ¡

```bash
# æ­£ç¡®çš„å‘½ä»¤æ ¼å¼ï¼ˆæœ¬åœ°è¿è¡Œï¼Œäº‘ç«¯æ‰§è¡Œï¼‰
# æ–¹å¼1: ä½¿ç”¨é¡¹ç›®çš„æ¨¡å—åŒ–æ¶æ„ï¼ˆæ¨èï¼‰
uv run hf jobs run --flavor a10g-small --secrets HF_TOKEN pytorch/pytorch:2.6.0-cuda12.4-cudnn9-devel -- bash -c "apt-get update && apt-get install -y git && git clone https://github.com/layue13/ft.git && cd ft && pip install uv && uv sync && uv run python scripts/train.py --config configs/training_config_public.yaml"

# æ–¹å¼2: å¤šè¡Œå‘½ä»¤ï¼ˆå¦‚æœç»ˆç«¯æ”¯æŒï¼‰
uv run hf jobs run \
    --flavor a10g-small \
    --secrets HF_TOKEN \
    pytorch/pytorch:2.6.0-cuda12.4-cudnn9-devel \
    -- bash -c "apt-get update && apt-get install -y git && git clone https://github.com/layue13/ft.git && cd ft && pip install uv && uv sync && uv run python scripts/train.py --config configs/training_config_public.yaml"

# ğŸš€ **ç¬¬ä¸€æ€§åŸç†ï¼šæœ€ç®€æ–¹æ¡ˆ**

## æ–¹æ¡ˆAï¼šå†…è”UVè„šæœ¬ï¼ˆæ¨èï¼‰
```bash
uv run hf jobs uv --flavor a10g-small --secrets HF_TOKEN --script "
# /// script
# dependencies = ['transformers', 'datasets', 'peft', 'torch', 'accelerate']
# ///

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model, TaskType
from datasets import load_dataset

# Gemma-3-1b Tool Use å¾®è°ƒ
tokenizer = AutoTokenizer.from_pretrained('google/gemma2-1.1b-it', padding_side='right')
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained('google/gemma2-1.1b-it', torch_dtype=torch.bfloat16, device_map='auto', trust_remote_code=True)

# LoRA - é’ˆå¯¹Gemmaä¼˜åŒ–
lora_config = LoraConfig(r=16, lora_alpha=32, lora_dropout=0.1, task_type=TaskType.CAUSAL_LM, target_modules=['q_proj', 'v_proj', 'k_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'])
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# Tool Useæ•°æ®æ ¼å¼åŒ– - æ­£ç¡®æ ¼å¼
def format_tool_use(example):
    if 'trace' not in example or not example.get('tool_needed'): 
        return {'text': '<bos><start_of_turn>user\\nhello<end_of_turn>\\n<start_of_turn>model\\nHello!<end_of_turn><eos>'}
    conv = '<bos>'
    for msg in example['trace']:
        if msg.get('role') == 'user': conv += f'<start_of_turn>user\\n{msg.get(\"content\", \"\")}<end_of_turn>\\n'
    if example.get('tool_needed') and example.get('tool_name'):
        tool_call = f'<tool_call>\\n{{\\n \"tool_name\": \"{example[\"tool_name\"]}\",\\n \"args\": {{}}\\n}}\\n</tool_call>'
        conv += f'<start_of_turn>model\\n{tool_call}<end_of_turn>'
    return {'text': conv + '<eos>'}

dataset = load_dataset('shawhin/tool-use-finetuning', split='train[:200]').map(format_tool_use)
tokenized = dataset.map(lambda x: tokenizer(x['text'], truncation=True, padding='max_length', max_length=512), batched=True, remove_columns=dataset.column_names)
tokenized = tokenized.add_column('labels', tokenized['input_ids'])

# è®­ç»ƒ - Tool Useä¼˜åŒ–å‚æ•°
trainer = Trainer(
    model=model,
    args=TrainingArguments(output_dir='./gemma3-tool-use', num_train_epochs=2, per_device_train_batch_size=1, gradient_accumulation_steps=4, learning_rate=2e-5, warmup_ratio=0.1, logging_steps=10, save_strategy='epoch', push_to_hub=True, hub_model_id='gemma3-1b-tool-use', bf16=True, gradient_checkpointing=True, remove_unused_columns=False),
    train_dataset=tokenized,
    tokenizer=tokenizer
)
trainer.train()
print('ğŸ‰ è®­ç»ƒå®Œæˆï¼')
"
```

## æ–¹æ¡ˆBï¼šä½¿ç”¨æç®€è„šæœ¬
```bash
uv run hf jobs run --flavor a10g-small --secrets HF_TOKEN pytorch/pytorch:2.6.0-cuda12.4-cudnn9-devel -- bash -c "git clone https://github.com/layue13/ft.git && cd ft && python simple_train.py"
```

## ğŸ¯ **Gemma-3-1b Tool Use ä¼˜åŠ¿**

âœ… **ä¸“ä¸šç›®æ ‡**ï¼š
- ğŸ¤– Gemma-3-1b: ä¼˜ç§€çš„å°æ¨¡å‹
- ğŸ›  Tool Use: å·¥å…·è°ƒç”¨èƒ½åŠ›
- ğŸ“Š shawhin/tool-use-finetuning: ä¸“ä¸šæ•°æ®é›†
- ğŸ¯ XMLæ ¼å¼: <tool_call>æ ‡å‡†

âœ… **ä¼˜åŒ–è®¾ç½®**ï¼š
- LoRA r=16: é€‚åˆå·¥å…·è°ƒç”¨çš„å¤æ‚æ€§
- ç›®æ ‡æ¨¡å—: Gemmaå…¨æ³¨æ„åŠ›å±‚
- åºåˆ—é•¿åº¦512: æ”¯æŒå¤æ‚å·¥å…·è°ƒç”¨
- å­¦ä¹ ç‡2e-5: ä¿æŠ¤é¢„è®­ç»ƒçŸ¥è¯†

âœ… **äº‘ç«¯è®­ç»ƒ**ï¼š
- 200è®­ç»ƒæ ·æœ¬: å¿«é€ŸéªŒè¯
- 2ä¸ªepoch: å……åˆ†å­¦ä¹ 
- A10G GPU: é«˜æ•ˆè®­ç»ƒ
- 15-20åˆ†é’Ÿå®Œæˆ

ğŸ’° **æˆæœ¬ä¼°ç®—**ï¼š15-20åˆ†é’Ÿè®­ç»ƒ â‰ˆ $0.50
```

### 4. ç›‘æ§ä»»åŠ¡

```bash
# æŸ¥çœ‹æ‰€æœ‰ä»»åŠ¡
uv run hf jobs ps

# æŸ¥çœ‹ç‰¹å®šä»»åŠ¡è¯¦æƒ…
uv run hf jobs inspect <job-id>

# æŸ¥çœ‹ä»»åŠ¡æ—¥å¿—
uv run hf jobs logs <job-id>

# å–æ¶ˆä»»åŠ¡
uv run hf jobs cancel <job-id>
```

## ğŸ’° è´¹ç”¨ä¼°ç®—

| ç¡¬ä»¶ç±»å‹ | æ¯å°æ—¶è´¹ç”¨ | é€‚ç”¨åœºæ™¯ |
|----------|------------|----------|
| t4-small | ~$0.50 | å°æ¨¡å‹å¾®è°ƒ |
| a10g-small | ~$1.50 | ä¸­ç­‰æ¨¡å‹å¾®è°ƒ |  
| a100-large | ~$4.00 | å¤§æ¨¡å‹è®­ç»ƒ |

**å¯¹äºGemma-3-1bå¾®è°ƒï¼Œæ¨èä½¿ç”¨ `a10g-small`ï¼Œé¢„ä¼°æˆæœ¬ï¼š**
- 2å°æ—¶è®­ç»ƒ â‰ˆ $3
- æ¯”è´­ä¹°GPUä¾¿å®œå¾—å¤šï¼

## ğŸ“ ä½¿ç”¨æˆ‘ä»¬çš„è®­ç»ƒè„šæœ¬

é¡¹ç›®ä¸­å·²åŒ…å« `hf_jobs_train.py`ï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºHF Jobsä¼˜åŒ–çš„è®­ç»ƒè„šæœ¬ï¼š

```bash
# æäº¤æˆ‘ä»¬çš„å¾®è°ƒä»»åŠ¡
hf jobs run --flavor a10g-small \
    --env HUGGINGFACE_HUB_TOKEN=$HF_TOKEN \
    pytorch/pytorch:2.6.0-cuda12.4-cudnn9-devel \
    bash -c "
        git clone https://github.com/layue13/ft.git && 
        cd ft && 
        pip install transformers datasets peft accelerate torch && 
        python hf_jobs_train.py
    "
```

## âœ… ä¼˜åŠ¿

1. **æ— ç¡¬ä»¶éœ€æ±‚**ï¼šæ— éœ€è´­ä¹°æ˜‚è´µGPU
2. **å¼¹æ€§æ‰©å±•**ï¼šæŒ‰éœ€é€‰æ‹©ç¡¬ä»¶è§„æ ¼
3. **è‡ªåŠ¨ä¿å­˜**ï¼šè®­ç»ƒå®Œæˆåæ¨¡å‹è‡ªåŠ¨ä¸Šä¼ åˆ°ä½ çš„Hub
4. **ä¸“ä¸šç¯å¢ƒ**ï¼šé¢„é…ç½®çš„PyTorchç¯å¢ƒ
5. **æŒ‰éœ€ä»˜è´¹**ï¼šåªä¸ºå®é™…ä½¿ç”¨æ—¶é—´ä»˜è´¹

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **gitå‘½ä»¤æœªæ‰¾åˆ°** ğŸ”§
   ```
   bash: line 1: git: command not found
   ```
   **è§£å†³æ–¹æ¡ˆ**: ä½¿ç”¨æ­£ç¡®çš„å‘½ä»¤æ ¼å¼ï¼ˆæ³¨æ„ `--` åˆ†éš”ç¬¦ï¼‰:
   ```bash
   uv run hf jobs run --flavor a10g-small --secrets HF_TOKEN pytorch/pytorch:2.6.0-cuda12.4-cudnn9-devel \
       -- bash -c "apt-get update && apt-get install -y git && git clone https://github.com/layue13/ft.git && cd ft && pip install uv && uv run python hf_jobs_train.py"
   ```

1b. **å‘½ä»¤è§£æé”™è¯¯** ğŸ”§
   ```
   usage: hf <command> [<args>] jobs run: error: the following arguments are required: image
   zsh: no such file or directory: pytorch/pytorch:2.6.0-cuda12.4-cudnn9-devel
   ```
   **åŸå› **: å¤šè¡Œå‘½ä»¤åœ¨æŸäº›ç»ˆç«¯ä¸­è¢«é”™è¯¯è§£æ
   **è§£å†³æ–¹æ¡ˆ**: ä½¿ç”¨é¡¹ç›®çš„æ¨¡å—åŒ–æ¶æ„:
   ```bash
   uv run hf jobs run --flavor a10g-small --secrets HF_TOKEN pytorch/pytorch:2.6.0-cuda12.4-cudnn9-devel -- bash -c "apt-get update && apt-get install -y git && git clone https://github.com/layue13/ft.git && cd ft && pip install uv && uv sync && uv run python scripts/train.py --config configs/training_config_public.yaml"
   ```

1c. **åº“ç‰ˆæœ¬å…¼å®¹æ€§é”™è¯¯** ğŸ”§
   ```
   TypeError: Accelerator.unwrap_model() got an unexpected keyword argument 'keep_torch_compile'
   ```
   **æ ¹æœ¬åŸå› **: ä½¿ç”¨äº†ç‹¬ç«‹çš„hf_jobs_train.pyè„šæœ¬è€Œä¸æ˜¯é¡¹ç›®çš„æ¨¡å—åŒ–æ¶æ„
   **è§£å†³æ–¹æ¡ˆ**: 
   - âœ… ä½¿ç”¨é¡¹ç›®çš„scripts/train.pyï¼ˆå·²é€šè¿‡æœ¬åœ°æµ‹è¯•ï¼‰
   - âœ… ä½¿ç”¨uv syncè‡ªåŠ¨ç®¡ç†ä¾èµ–ç‰ˆæœ¬
   - âœ… ä½¿ç”¨é…ç½®æ–‡ä»¶è€Œä¸æ˜¯ç¡¬ç¼–ç å‚æ•°
   
   **ä¿®å¤åçš„å‘½ä»¤**: è§ä¸Šæ–¹æ–°å‘½ä»¤æ ¼å¼

2. **è®¤è¯å¤±è´¥**
   ```bash
   huggingface-cli login --token <your-token>
   ```

3. **ä»»åŠ¡å¤±è´¥**
   ```bash
   # æŸ¥çœ‹è¯¦ç»†æ—¥å¿—
   uv run hf jobs logs <job-id>
   ```

4. **å†…å­˜ä¸è¶³**
   - å‡çº§åˆ°æ›´å¤§çš„GPUè§„æ ¼
   - å‡å°‘batch_size
   - å¯ç”¨gradient_checkpointing

### è°ƒè¯•æŠ€å·§

```bash
# æµ‹è¯•è„šæœ¬ï¼ˆä¸å¯åŠ¨å®é™…è®­ç»ƒï¼‰
hf jobs run --flavor cpu-basic python:3.11 \
    python -c "import torch; print('ç¯å¢ƒæµ‹è¯•æˆåŠŸ')"
```

## ğŸ¯ ä¸‹ä¸€æ­¥

1. è·å–HF Proè´¦æˆ·
2. å…‹éš†è¿™ä¸ªä»“åº“åˆ°GitHubï¼ˆå·²å®Œæˆï¼‰
3. ä½¿ç”¨ä¸Šè¿°å‘½ä»¤æäº¤ä½ çš„å¾®è°ƒä»»åŠ¡
4. ç›‘æ§è®­ç»ƒè¿›åº¦
5. è®­ç»ƒå®Œæˆååœ¨HF Hubä¸‹è½½ä½ çš„å¾®è°ƒæ¨¡å‹

**å¼€å§‹ä½ çš„äº‘ç«¯å¾®è°ƒä¹‹æ—…å§ï¼** ğŸš€
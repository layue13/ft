#!/usr/bin/env python3
"""
Gemma-3-1b Tool Use å¾®è°ƒè„šæœ¬ - HF Jobsç‰ˆæœ¬
åŸºäºç¬¬ä¸€æ€§åŸç†ï¼šæ¨¡å‹ + æ•°æ® + è®­ç»ƒå¾ªç¯
ç›®æ ‡ï¼šè®©Gemma-3-1bæ”¯æŒå·¥å…·è°ƒç”¨
"""

import os
import subprocess
import torch
from transformers import (
    AutoModelForCausalLM, AutoTokenizer, 
    TrainingArguments, Trainer, DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import load_dataset, Dataset
from huggingface_hub import login
import warnings

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings("ignore", category=FutureWarning, module="datasets")
warnings.filterwarnings("ignore", category=SyntaxWarning, module="peft")
warnings.filterwarnings("ignore", category=FutureWarning, module="huggingface_hub")
warnings.filterwarnings("ignore", category=UserWarning, module="torch.utils.data.dataloader")

def setup_environment():
    """è®¾ç½®ç¯å¢ƒ"""
    print("ğŸ”§ è®¾ç½®ç¯å¢ƒ...")
    
    # ä½¿ç”¨uvå®‰è£…ä¾èµ–
    print("ğŸ“¦ ä½¿ç”¨uvå®‰è£…ä¾èµ–...")
    subprocess.check_call(["uv", "sync"])

def main():
    print("ğŸš€ å¼€å§‹Gemma-3-1b Tool Useå¾®è°ƒ (HF Jobsç‰ˆæœ¬)...")
    
    # è®¾ç½®ç¯å¢ƒ
    setup_environment()
    
    # ç™»å½•Hugging Face
    if os.getenv("HF_TOKEN"):
        login(token=os.getenv("HF_TOKEN"))
        print("âœ… å·²ç™»å½•Hugging Face")
    
    # æ£€æŸ¥è®¾å¤‡æ”¯æŒ
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ£€æŸ¥bf16æ”¯æŒ
    bf16_supported = torch.cuda.is_available() and torch.cuda.is_bf16_supported()
    print(f"ğŸ”§ bf16æ”¯æŒ: {bf16_supported}")
    
    # 1. æ¨¡å‹å’Œåˆ†è¯å™¨ - Gemma-3-1b
    model_name = "google/gemma-3-1b-it"  # ä½¿ç”¨Gemma-3-1bæ¨¡å‹
    print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {model_name}")
    
    # åŠ è½½tokenizerï¼Œä½¿ç”¨å®‰å…¨é…ç½®
    tokenizer = AutoTokenizer.from_pretrained(
        model_name, 
        padding_side="right",
        trust_remote_code=True
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # åŠ è½½æ¨¡å‹ï¼Œä½¿ç”¨eageræ³¨æ„åŠ›æœºåˆ¶
    torch_dtype = torch.bfloat16 if bf16_supported else torch.float32
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch_dtype,
        device_map="auto" if device == "cuda" else None,
        trust_remote_code=True,
        attn_implementation="eager"  # ä½¿ç”¨eageræ³¨æ„åŠ›æœºåˆ¶
    )
    
    # 2. LoRAé…ç½® - é’ˆå¯¹Gemmaæ¨¡å‹
    lora_config = LoraConfig(
        r=16, lora_alpha=32, lora_dropout=0.1,
        task_type=TaskType.CAUSAL_LM,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    )
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # 3. Tool Useæ•°æ® - ä½¿ç”¨çœŸå®çš„å·¥å…·è°ƒç”¨æ•°æ®é›†
    print("ğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®...")
    
    # åŠ è½½çœŸå®çš„å·¥å…·è°ƒç”¨æ•°æ®é›†
    dataset = load_dataset("shawhin/tool-use-finetuning", split="train[:200]")
    print(f"ğŸ“¦ åŠ è½½æ•°æ®é›†: {len(dataset)} ä¸ªæ ·æœ¬")
    
    def format_tool_use_data(example):
        """æ ¼å¼åŒ–å·¥å…·è°ƒç”¨æ•°æ®ä¸ºGemmaå¯¹è¯æ ¼å¼"""
        if "trace" not in example or not example.get("tool_needed"):
            return {"text": "<bos><start_of_turn>user\nhello<end_of_turn>\n<start_of_turn>model\nHello! How can I help you?<end_of_turn><eos>"}
        
        conversation = "<bos>"
        
        # å¤„ç†å¯¹è¯å†å²
        for msg in example["trace"]:
            role = msg.get("role", "")
            content = msg.get("content", "")
            
            if role == "user":
                conversation += f"<start_of_turn>user\n{content}<end_of_turn>\n"
        
        # æ·»åŠ å·¥å…·è°ƒç”¨å“åº”
        if example.get("tool_needed") and example.get("tool_name"):
            tool_call = f'<tool_call>\n{{\n "tool_name": "{example["tool_name"]}",\n "args": {{}}\n}}\n</tool_call>'
            conversation += f"<start_of_turn>model\n{tool_call}<end_of_turn>"
        
        conversation += "<eos>"
        return {"text": conversation}
    
    # æ ¼å¼åŒ–æ•°æ®é›†
    dataset = dataset.map(format_tool_use_data)
    print(f"âœ… æ•°æ®æ ¼å¼åŒ–å®Œæˆ")
    
    def tokenize(examples):
        return tokenizer(
            examples["text"], 
            truncation=True, 
            padding="max_length",
            max_length=512,  # Gemmaéœ€è¦æ›´é•¿åºåˆ—
            return_tensors="pt"
        )
    
    tokenized = dataset.map(tokenize, batched=True, remove_columns=dataset.column_names)
    tokenized = tokenized.add_column("labels", tokenized["input_ids"])
    
    # 4. æ•°æ®æ•´ç†å™¨ - å¤„ç†æ‰¹å¤„ç†
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,  # ä½¿ç”¨å› æœè¯­è¨€å»ºæ¨¡
    )
    
    # 5. è®­ç»ƒå‚æ•° - HF Jobsä¼˜åŒ–
    training_args = TrainingArguments(
        output_dir="./gemma3-tool-use",
        num_train_epochs=3,  # å¢åŠ è®­ç»ƒè½®æ•°
        per_device_train_batch_size=2,  # å¢åŠ batch size
        gradient_accumulation_steps=8,  # å¢åŠ æ¢¯åº¦ç´¯ç§¯
        learning_rate=2e-5,  # è¾ƒä½å­¦ä¹ ç‡é¿å…ç ´åé¢„è®­ç»ƒçŸ¥è¯†
        warmup_ratio=0.1,
        logging_steps=5,
        save_strategy="epoch",
        save_total_limit=3,
        push_to_hub=True,  # æ¨é€åˆ°Hub
        hub_model_id="gemma3-1b-tool-use",  # æŒ‡å®šHubæ¨¡å‹å
        report_to="wandb",  # ä½¿ç”¨wandbè®°å½•
        remove_unused_columns=False,
        dataloader_num_workers=2,  # å¢åŠ æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹
        bf16=bf16_supported,
        gradient_checkpointing=True,  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
        dataloader_pin_memory=True,  # å¯ç”¨pin_memory
    )
    
    # 6. è®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized,
        data_collator=data_collator,
    )
    
    # 7. å¼€å§‹è®­ç»ƒ
    print("âœ¨ å¼€å§‹Gemma-3-1b Tool Useå¾®è°ƒ...")
    print(f"ğŸ“Š è®­ç»ƒæ ·æœ¬: {len(tokenized)}")
    print(f"ğŸ¯ ç›®æ ‡: è®©Gemma-3-1bå­¦ä¼šå·¥å…·è°ƒç”¨")
    print(f"âš™ï¸ è®­ç»ƒé…ç½®: batch_size={training_args.per_device_train_batch_size}, "
          f"gradient_accumulation={training_args.gradient_accumulation_steps}, "
          f"learning_rate={training_args.learning_rate}")
    
    trainer.train()
    
    # 8. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    trainer.save_model()
    tokenizer.save_pretrained("./gemma3-tool-use")
    
    # 9. æ¨é€åˆ°Hub
    if os.getenv("HF_TOKEN"):
        trainer.push_to_hub()
        print("ğŸ“¤ æ¨¡å‹å·²æ¨é€åˆ°Hugging Face Hub")
    
    print("ğŸ‰ Gemma-3-1b Tool Useå¾®è°ƒå®Œæˆï¼")
    print("ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ° ./gemma3-tool-use")
    print("ğŸ›  ç°åœ¨å¯ä»¥ä½¿ç”¨å·¥å…·è°ƒç”¨åŠŸèƒ½äº†ï¼")

if __name__ == "__main__":
    main()

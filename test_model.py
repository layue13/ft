#!/usr/bin/env python3
"""
æµ‹è¯•æ¨¡å‹è„šæœ¬ - éªŒè¯å·¥å…·è°ƒç”¨åŠŸèƒ½
"""

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

def test_model():
    print("ğŸš€ å¼€å§‹æµ‹è¯•æ¨¡å‹...")
    
    model_path = "./gemma3-1b-tool-use-merged"
    
    try:
        print("ğŸ“¦ åŠ è½½æ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        
        # æµ‹è¯•ç”¨ä¾‹
        test_cases = [
            "å¸®æˆ‘æŸ¥è¯¢åŒ—äº¬çš„å¤©æ°”",
            "è¯·å¸®æˆ‘è®¡ç®—23+45",
            "æˆ‘æƒ³çŸ¥é“ä»Šå¤©çš„æ—¥æœŸ",
            "å¸®æˆ‘æœç´¢Pythonæ•™ç¨‹"
        ]
        
        print("\nğŸ§ª å¼€å§‹æµ‹è¯•å·¥å…·è°ƒç”¨...")
        
        for i, prompt in enumerate(test_cases, 1):
            print(f"\n--- æµ‹è¯• {i}: {prompt} ---")
            
            # æ ¼å¼åŒ–è¾“å…¥
            formatted_prompt = f"<bos><start_of_turn>user\n{prompt}<end_of_turn>\n<start_of_turn>model\n"
            
            # ç¼–ç 
            inputs = tokenizer(formatted_prompt, return_tensors="pt")
            
            # ç”Ÿæˆ
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_length=inputs['input_ids'].shape[1] + 200,
                    temperature=0.7,
                    top_p=0.9,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            # è§£ç 
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # æå–æ¨¡å‹å›å¤éƒ¨åˆ†
            if "<start_of_turn>model" in response:
                model_response = response.split("<start_of_turn>model")[1].split("<end_of_turn>")[0].strip()
                print(f"åŠ©æ‰‹: {model_response}")
            else:
                print(f"å®Œæ•´å›å¤: {response}")
        
        print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
        print("ğŸ’¡ å¦‚æœçœ‹åˆ°<tool_call>æ ¼å¼çš„å›å¤ï¼Œè¯´æ˜æ¨¡å‹è®­ç»ƒæˆåŠŸï¼")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        print("ğŸ’¡ è¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´")

if __name__ == "__main__":
    test_model()

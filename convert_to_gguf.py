#!/usr/bin/env python3
"""
GGUFæ ¼å¼è½¬æ¢è„šæœ¬
å°†åˆå¹¶åçš„æ¨¡å‹è½¬æ¢ä¸ºGGUFæ ¼å¼ï¼Œè·å¾—æœ€ä½³æ€§èƒ½å’Œå…¼å®¹æ€§
"""

import os
import sys
import subprocess
from pathlib import Path

def check_dependencies():
    """æ£€æŸ¥å¿…è¦çš„ä¾èµ–"""
    print("ğŸ” æ£€æŸ¥ä¾èµ–...")
    
    try:
        import transformers
        print("âœ… transformers")
    except ImportError:
        print("âŒ transformers æœªå®‰è£…")
        return False
    
    try:
        # æ£€æŸ¥transformers-to-gguf
        result = subprocess.run(["transformers-to-gguf", "--help"], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            print("âœ… transformers-to-gguf")
        else:
            print("âŒ transformers-to-gguf æœªå®‰è£…")
            return False
    except FileNotFoundError:
        print("âŒ transformers-to-gguf æœªå®‰è£…")
        return False
    
    return True

def install_dependencies():
    """å®‰è£…å¿…è¦çš„ä¾èµ–"""
    print("ğŸ“¦ å®‰è£…ä¾èµ–...")
    
    try:
        subprocess.check_call([sys.executable, "-m", "pip", "install", "transformers-to-gguf"])
        print("âœ… transformers-to-gguf å®‰è£…æˆåŠŸ")
        return True
    except subprocess.CalledProcessError:
        print("âŒ å®‰è£…å¤±è´¥")
        return False

def convert_to_gguf(model_path, output_file, quantization="q4_k_m"):
    """è½¬æ¢ä¸ºGGUFæ ¼å¼"""
    print(f"ğŸ”„ å¼€å§‹GGUFè½¬æ¢...")
    print(f"ğŸ“¦ æºæ¨¡å‹: {model_path}")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
    print(f"ğŸ”§ é‡åŒ–ç±»å‹: {quantization}")
    
    # æ£€æŸ¥æºæ¨¡å‹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        print(f"âŒ æºæ¨¡å‹ä¸å­˜åœ¨: {model_path}")
        return False
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„æ–‡ä»¶
    required_files = ["config.json", "pytorch_model.bin", "tokenizer.json"]
    missing_files = [f for f in required_files if not os.path.exists(os.path.join(model_path, f))]
    
    if missing_files:
        print(f"âŒ æºæ¨¡å‹ç¼ºå°‘å¿…è¦æ–‡ä»¶: {missing_files}")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œ merge_lora.py åˆå¹¶LoRAæƒé‡")
        return False
    
    try:
        # ä½¿ç”¨transformers-to-ggufè½¬æ¢
        cmd = [
            "transformers-to-gguf",
            model_path,
            "--output", output_file,
            "--quantize", quantization
        ]
        
        print("ğŸ”„ æ‰§è¡Œè½¬æ¢å‘½ä»¤...")
        print(f"å‘½ä»¤: {' '.join(cmd)}")
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if result.returncode == 0:
            print("âœ… GGUFè½¬æ¢æˆåŠŸï¼")
            
            # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
            if os.path.exists(output_file):
                file_size = os.path.getsize(output_file) / (1024 * 1024)  # MB
                print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
                print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
            
            return True
        else:
            print("âŒ GGUFè½¬æ¢å¤±è´¥")
            print(f"é”™è¯¯è¾“å‡º: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"âŒ è½¬æ¢å¤±è´¥: {e}")
        return False

def create_usage_script(output_file):
    """åˆ›å»ºä½¿ç”¨è„šæœ¬"""
    print("ğŸ“ åˆ›å»ºä½¿ç”¨è„šæœ¬...")
    
    script_content = f"""#!/usr/bin/env python3
\"\"\"
GGUFæ¨¡å‹ä½¿ç”¨ç¤ºä¾‹
\"\"\"

import os
from llama_cpp import Llama

def load_gguf_model(model_path):
    \"\"\"åŠ è½½GGUFæ¨¡å‹\"\"\"
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {{model_path}}")
        return None
    
    try:
        # åŠ è½½æ¨¡å‹
        llm = Llama(
            model_path=model_path,
            n_ctx=4096,  # ä¸Šä¸‹æ–‡é•¿åº¦
            n_threads=4,  # CPUçº¿ç¨‹æ•°
            n_gpu_layers=0  # GPUå±‚æ•°ï¼Œæ ¹æ®ä½ çš„GPUè°ƒæ•´
        )
        print("âœ… GGUFæ¨¡å‹åŠ è½½æˆåŠŸ")
        return llm
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {{e}}")
        return None

def generate_tool_call(llm, prompt, max_tokens=512):
    \"\"\"ç”Ÿæˆå·¥å…·è°ƒç”¨å“åº”\"\"\"
    try:
        # æ ¼å¼åŒ–è¾“å…¥
        formatted_prompt = f"<bos><start_of_turn>user\\n{{prompt}}<end_of_turn>\\n<start_of_turn>model\\n"
        
        # ç”Ÿæˆå“åº”
        response = llm(
            formatted_prompt,
            max_tokens=max_tokens,
            temperature=0.7,
            top_p=0.9,
            stop=["<eos>", "</s>"]
        )
        
        return response['choices'][0]['text']
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¤±è´¥: {{e}}")
        return None

if __name__ == "__main__":
    # åŠ è½½æ¨¡å‹
    model_path = "{output_file}"
    llm = load_gguf_model(model_path)
    
    if llm:
        # æµ‹è¯•å·¥å…·è°ƒç”¨
        prompt = "å¸®æˆ‘æŸ¥è¯¢åŒ—äº¬çš„å¤©æ°”"
        print(f"\\nç”¨æˆ·: {{prompt}}")
        
        response = generate_tool_call(llm, prompt)
        if response:
            print(f"åŠ©æ‰‹: {{response}}")
        else:
            print("âŒ ç”Ÿæˆå¤±è´¥")
"""
    
    script_file = "gguf_inference.py"
    with open(script_file, "w") as f:
        f.write(script_content)
    
    print(f"âœ… ä½¿ç”¨è„šæœ¬å·²åˆ›å»º: {script_file}")

def main():
    print("ğŸš€ GGUFæ ¼å¼è½¬æ¢å·¥å…·")
    print("=" * 50)
    
    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        print("\nğŸ“¦ å®‰è£…ç¼ºå¤±çš„ä¾èµ–...")
        if not install_dependencies():
            print("âŒ ä¾èµ–å®‰è£…å¤±è´¥")
            return
        if not check_dependencies():
            print("âŒ ä¾èµ–æ£€æŸ¥å¤±è´¥")
            return
    
    print("\n" + "=" * 50)
    
    # è·å–è¾“å…¥
    model_path = input("è¯·è¾“å…¥æ¨¡å‹è·¯å¾„ (ä¾‹å¦‚: ./gemma3-1b-tool-use-merged): ").strip()
    if not model_path:
        print("âŒ æ¨¡å‹è·¯å¾„ä¸èƒ½ä¸ºç©º")
        return
    
    output_file = input("è¯·è¾“å…¥è¾“å‡ºæ–‡ä»¶ (é»˜è®¤: ./gemma3-1b-tool-use.gguf): ").strip()
    if not output_file:
        output_file = "./gemma3-1b-tool-use.gguf"
    
    # é‡åŒ–é€‰é¡¹
    print("\nğŸ”§ é€‰æ‹©é‡åŒ–ç±»å‹:")
    print("1. q4_k_m (æ¨è) - å¹³è¡¡è´¨é‡å’Œå¤§å°")
    print("2. q8_0 - é«˜è´¨é‡ï¼Œè¾ƒå¤§æ–‡ä»¶")
    print("3. q5_k_m - ä¸­ç­‰è´¨é‡")
    print("4. q3_k_m - å°æ–‡ä»¶ï¼Œè´¨é‡è¾ƒä½")
    
    quantization = input("è¯·é€‰æ‹©é‡åŒ–ç±»å‹ (é»˜è®¤: q4_k_m): ").strip()
    if not quantization:
        quantization = "q4_k_m"
    
    print("\n" + "=" * 50)
    
    # æ‰§è¡Œè½¬æ¢
    if convert_to_gguf(model_path, output_file, quantization):
        print("\nğŸ‰ GGUFè½¬æ¢æˆåŠŸï¼")
        print(f"ğŸ“ GGUFæ–‡ä»¶: {output_file}")
        
        # åˆ›å»ºä½¿ç”¨è„šæœ¬
        create_usage_script(output_file)
        
        print("\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
        print("1. å®‰è£…ä¾èµ–: pip install llama-cpp-python")
        print("2. è¿è¡Œç¤ºä¾‹: python gguf_inference.py")
        print("3. åœ¨LM Studioä¸­åŠ è½½GGUFæ–‡ä»¶")
        print("4. ä½¿ç”¨llama.cppè¿›è¡Œæ¨ç†")
    else:
        print("\nâŒ GGUFè½¬æ¢å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")

if __name__ == "__main__":
    main()

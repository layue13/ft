#!/usr/bin/env python3
"""
GGUFæ ¼å¼è½¬æ¢è„šæœ¬
ä½¿ç”¨llama.cppçš„convert_hf_to_gguf.pyå°†Hugging Faceæ¨¡å‹è½¬æ¢ä¸ºGGUFæ ¼å¼
æ”¯æŒPEFTæ¨¡å‹å’Œå¤šç§é‡åŒ–é€‰é¡¹
"""

import os
import sys
import subprocess
import shutil
import platform
from pathlib import Path

def check_dependencies():
    """æ£€æŸ¥å¿…è¦çš„ä¾èµ–"""
    print("ğŸ” æ£€æŸ¥ä¾èµ–...")
    
    # æ£€æŸ¥git
    try:
        subprocess.run(["git", "--version"], check=True, capture_output=True)
        print("âœ… git")
    except (subprocess.CalledProcessError, FileNotFoundError):
        print("âŒ git æœªå®‰è£…")
        return False
    
    return True

def check_llama_cpp():
    """æ£€æŸ¥llama-cpp-pythonæ˜¯å¦å·²å®‰è£…"""
    try:
        import llama_cpp
        print("âœ… llama-cpp-pythonå·²å®‰è£…")
        return True
    except ImportError:
        print("âŒ llama-cpp-pythonæœªå®‰è£…")
        return False

def install_llama_cpp_simple():
    """ç®€å•å®‰è£…llama-cpp-python"""
    print("ğŸ“¦ å®‰è£…llama-cpp-python...")
    
    # å°è¯•å¤šç§å®‰è£…æ–¹å¼
    install_methods = [
        # æ–¹å¼1: uvå®‰è£…
        (["uv", "add", "llama-cpp-python"], "uv"),
        # æ–¹å¼2: pipå®‰è£…
        ([sys.executable, "-m", "pip", "install", "llama-cpp-python"], "pip"),
    ]
    
    for cmd, method in install_methods:
        try:
            print(f"ğŸ”§ å°è¯•ä½¿ç”¨{method}å®‰è£…...")
            subprocess.run(cmd, check=True)
            print(f"âœ… {method}å®‰è£…æˆåŠŸ")
            return True
        except (subprocess.CalledProcessError, FileNotFoundError):
            print(f"âŒ {method}å®‰è£…å¤±è´¥")
            continue
    
    print("âŒ æ‰€æœ‰å®‰è£…æ–¹å¼éƒ½å¤±è´¥")
    return False

def install_llama_cpp():
    """å®‰è£…llama.cpp - æ”¯æŒå¤šç§æ–¹å¼"""
    print("ğŸ“¦ å®‰è£…llama.cpp...")
    
    # é¦–å…ˆæ£€æŸ¥æ˜¯å¦å·²ç»å®‰è£…
    if check_llama_cpp():
        return True
    
    # å°è¯•ç®€å•å®‰è£…
    if install_llama_cpp_simple():
        return True
    
    # å¦‚æœç®€å•å®‰è£…å¤±è´¥ï¼Œå°è¯•ä»æºç å®‰è£…
    if os.path.exists("llama.cpp"):
        print("âœ… llama.cppå·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")
        return True
    
    try:
        # å…‹éš†llama.cppä»“åº“
        subprocess.run([
            "git", "clone", "https://github.com/ggml-org/llama.cpp.git"
        ], check=True)
        print("âœ… å·²å…‹éš†llama.cppä»“åº“")
        
        # å°è¯•ä¸åŒçš„æ„å»ºæ–¹æ³•
        os.chdir("llama.cpp")
        
        # æ–¹æ³•1: å°è¯•ä½¿ç”¨CMake
        try:
            print("ğŸ”§ å°è¯•ä½¿ç”¨CMakeæ„å»º...")
            subprocess.run(["cmake", "--version"], check=True, capture_output=True)
            
            # åˆ›å»ºbuildç›®å½•
            os.makedirs("build", exist_ok=True)
            os.chdir("build")
            
            # é…ç½®å’Œæ„å»º
            subprocess.run(["cmake", ".."], check=True)
            subprocess.run(["cmake", "--build", ".", "--config", "Release"], check=True)
            
            print("âœ… CMakeæ„å»ºæˆåŠŸ")
            os.chdir("..")
            
        except (subprocess.CalledProcessError, FileNotFoundError):
            print("âš ï¸ CMakeæ„å»ºå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨make...")
            
            # æ–¹æ³•2: å°è¯•ä½¿ç”¨make (æ—§ç‰ˆæœ¬)
            try:
                subprocess.run(["make"], check=True)
                print("âœ… Makeæ„å»ºæˆåŠŸ")
            except subprocess.CalledProcessError:
                print("âš ï¸ Makeæ„å»ºä¹Ÿå¤±è´¥")
                os.chdir("..")
                return False
        
        os.chdir("..")
        return True
        
    except subprocess.CalledProcessError as e:
        print(f"âŒ å®‰è£…å¤±è´¥: {e}")
        return False

def is_peft_model(model_path):
    """æ£€æŸ¥æ˜¯å¦ä¸ºPEFTæ¨¡å‹"""
    try:
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨adapter_config.json
        adapter_config = os.path.join(model_path, "adapter_config.json")
        if os.path.exists(adapter_config):
            return True
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨adapter_model.bin
        adapter_model = os.path.join(model_path, "adapter_model.bin")
        if os.path.exists(adapter_model):
            return True
        
        return False
    except:
        return False

def merge_lora_weights(model_path, output_dir):
    """åˆå¹¶LoRAæƒé‡"""
    print(f"ğŸ”§ æ£€æµ‹åˆ°PEFTæ¨¡å‹ï¼Œå¼€å§‹åˆå¹¶LoRAæƒé‡...")
    print(f"ğŸ“¦ æºæ¨¡å‹: {model_path}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer
        from peft import PeftModel
        
        print("ğŸ“¦ åŠ è½½åŸºç¡€æ¨¡å‹ (google/gemma-3-1b-it)...")
        base_model = AutoModelForCausalLM.from_pretrained(
            "google/gemma-3-1b-it",
            torch_dtype="auto",
            device_map="auto"
        )
        tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-1b-it")
        
        print("ğŸ“¦ åŠ è½½LoRAæƒé‡...")
        model = PeftModel.from_pretrained(base_model, model_path)
        
        print("ğŸ”§ åˆå¹¶æƒé‡...")
        merged_model = model.merge_and_unload()
        
        print("ğŸ’¾ ä¿å­˜åˆå¹¶åçš„æ¨¡å‹...")
        merged_model.save_pretrained(output_dir)
        tokenizer.save_pretrained(output_dir)
        
        print(f"âœ… LoRAæƒé‡åˆå¹¶å®Œæˆï¼")
        print(f"ğŸ“ åˆå¹¶åçš„æ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}")
        
        return True
        
    except Exception as e:
        print(f"âŒ åˆå¹¶å¤±è´¥: {e}")
        return False

def convert_to_gguf(model_path, output_file, quantization="q4_k_m"):
    """è½¬æ¢ä¸ºGGUFæ ¼å¼"""
    print(f"ğŸ”„ å¼€å§‹è½¬æ¢: {model_path} -> {output_file}")
    
    # æ£€æŸ¥è½¬æ¢è„šæœ¬
    convert_script = "llama.cpp/convert_hf_to_gguf.py"
    
    # å¦‚æœllama.cppä¸å­˜åœ¨ï¼Œå°è¯•ä½¿ç”¨pipå®‰è£…çš„ç‰ˆæœ¬
    if not os.path.exists("llama.cpp"):
        print("ğŸ“¦ llama.cppä¸å­˜åœ¨ï¼Œå°è¯•ä½¿ç”¨pipå®‰è£…çš„ç‰ˆæœ¬...")
        try:
            import llama_cpp
            print("âœ… æ‰¾åˆ°llama-cpp-python")
            
            # ä½¿ç”¨transformersçš„è½¬æ¢åŠŸèƒ½
            return convert_with_transformers(model_path, output_file, quantization)
        except ImportError:
            print("âŒ æœªæ‰¾åˆ°llama-cpp-pythonï¼Œè¯·å…ˆå®‰è£…")
            print("ğŸ’¡ å»ºè®®è¿è¡Œ: uv add llama-cpp-python")
            return False
    
    if not os.path.exists(convert_script):
        print(f"âŒ è½¬æ¢è„šæœ¬ä¸å­˜åœ¨: {convert_script}")
        return False
    
    try:
        # è¿è¡Œè½¬æ¢
        cmd = [
            sys.executable, convert_script,
            model_path,
            "--outfile", output_file,
            "--outtype", quantization
        ]
        
        print(f"ğŸš€ æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        subprocess.run(cmd, check=True)
        
        print(f"âœ… è½¬æ¢å®Œæˆ: {output_file}")
        return True
        
    except subprocess.CalledProcessError as e:
        print(f"âŒ è½¬æ¢å¤±è´¥: {e}")
        return False

def convert_with_transformers(model_path, output_file, quantization="q4_k_m"):
    """ä½¿ç”¨transformersè¿›è¡Œè½¬æ¢"""
    print("ğŸ”„ ä½¿ç”¨transformersè¿›è¡Œè½¬æ¢...")
    
    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer
        import torch
        
        print("ğŸ“¦ åŠ è½½æ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        print("ğŸ’¾ ä¿å­˜ä¸ºGGUFå…¼å®¹æ ¼å¼...")
        # ä¿å­˜ä¸ºtransformersæ ¼å¼ï¼Œç„¶åå¯ä»¥ä½¿ç”¨å…¶ä»–å·¥å…·è½¬æ¢
        temp_dir = "temp_for_gguf"
        model.save_pretrained(temp_dir)
        tokenizer.save_pretrained(temp_dir)
        
        print("âœ… æ¨¡å‹å·²ä¿å­˜ä¸ºå…¼å®¹æ ¼å¼")
        print(f"ğŸ“ ä¸´æ—¶ç›®å½•: {temp_dir}")
        print("ğŸ’¡ è¯·ä½¿ç”¨å…¶ä»–å·¥å…·(å¦‚llama.cpp)å°†ä¸´æ—¶ç›®å½•è½¬æ¢ä¸ºGGUFæ ¼å¼")
        
        return True
        
    except Exception as e:
        print(f"âŒ è½¬æ¢å¤±è´¥: {e}")
        return False

def create_usage_script(output_file):
    """åˆ›å»ºä½¿ç”¨ç¤ºä¾‹è„šæœ¬"""
    print("ğŸ“ åˆ›å»ºä½¿ç”¨ç¤ºä¾‹è„šæœ¬...")
    
    usage_script = f"""
#!/usr/bin/env python3
\"\"\"
GGUFæ¨¡å‹ä½¿ç”¨ç¤ºä¾‹
\"\"\"

import os
import subprocess
import sys

def run_llama_cpp(model_path, prompt, max_tokens=512):
    \"\"\"ä½¿ç”¨llama.cppè¿è¡ŒGGUFæ¨¡å‹\"\"\"
    
    # å°è¯•å¤šç§è¿è¡Œæ–¹å¼
    commands = [
        # æ–¹å¼1: æœ¬åœ°llama.cpp
        ["./llama.cpp/main", "-m", model_path, "-n", str(max_tokens), "-p", prompt],
        # æ–¹å¼2: ç³»ç»Ÿå®‰è£…çš„llama.cpp
        ["llama-cpp", "-m", model_path, "-n", str(max_tokens), "-p", prompt],
        # æ–¹å¼3: pythonåŒ…
        [sys.executable, "-m", "llama_cpp", "-m", model_path, "-n", str(max_tokens), "-p", prompt]
    ]
    
    for cmd in commands:
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            return result.stdout.strip()
        except (subprocess.CalledProcessError, FileNotFoundError):
            continue
    
    print("âŒ æ‰€æœ‰è¿è¡Œæ–¹å¼éƒ½å¤±è´¥")
    return None

def main():
    model_path = "{output_file}"
    
    if not os.path.exists(model_path):
        print(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {{model_path}}")
        return
    
    # ç¤ºä¾‹æç¤º
    prompt = "å¸®æˆ‘æŸ¥è¯¢åŒ—äº¬çš„å¤©æ°”"
    
    print(f"ç”¨æˆ·: {{prompt}}")
    print("åŠ©æ‰‹: ", end="")
    
    response = run_llama_cpp(model_path, prompt)
    if response:
        print(response)
    else:
        print("ç”Ÿæˆå¤±è´¥")

if __name__ == "__main__":
    main()
"""
    
    with open("run_gguf_model.py", "w") as f:
        f.write(usage_script)
    
    print("âœ… ä½¿ç”¨ç¤ºä¾‹è„šæœ¬å·²åˆ›å»º: run_gguf_model.py")

def main():
    print("ğŸš€ GGUFæ ¼å¼è½¬æ¢å·¥å…·")
    print("=" * 50)
    
    # 1. æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        print("\nâŒ ä¾èµ–æ£€æŸ¥å¤±è´¥ï¼Œè¯·å®‰è£…git")
        return
    
    # 2. æ£€æŸ¥llama-cpp-python
    if not check_llama_cpp():
        print("\nğŸ“¦ éœ€è¦å®‰è£…llama-cpp-python...")
        if not install_llama_cpp():
            print("\nâŒ llama.cppå®‰è£…å¤±è´¥")
            print("\nğŸ’¡ æ‰‹åŠ¨å®‰è£…é€‰é¡¹:")
            print("1. ä½¿ç”¨uv: uv add llama-cpp-python")
            print("2. ä½¿ç”¨pip: pip install llama-cpp-python")
            print("3. è¿è¡Œå®‰è£…è„šæœ¬: python install_llama_cpp.py")
            return
    
    # 3. è·å–è¾“å…¥
    print("\nğŸ“ é…ç½®è½¬æ¢å‚æ•°:")
    
    model_path = input("è¯·è¾“å…¥æ¨¡å‹è·¯å¾„ (ä¾‹å¦‚: ./gemma3-1b-tool-use-merged æˆ– layue13/gemma3-1b-tool-use): ").strip()
    if not model_path:
        model_path = "./gemma3-1b-tool-use-merged"
    
    output_file = input("è¯·è¾“å…¥è¾“å‡ºæ–‡ä»¶å (ä¾‹å¦‚: gemma3-1b-tool-use.gguf): ").strip()
    if not output_file:
        output_file = "gemma3-1b-tool-use.gguf"
    
    # é‡åŒ–é€‰é¡¹
    quantization_options = {
        "1": "q4_k_m",    # æ¨èï¼Œå¹³è¡¡å¤§å°å’Œæ€§èƒ½
        "2": "q8_0",      # é«˜è´¨é‡ï¼Œè¾ƒå¤§æ–‡ä»¶
        "3": "q5_k_m",    # ä¸­ç­‰è´¨é‡
        "4": "q3_k_m",    # å°æ–‡ä»¶ï¼Œè¾ƒä½è´¨é‡
    }
    
    print("\nğŸ”§ é€‰æ‹©é‡åŒ–ç±»å‹:")
    for key, value in quantization_options.items():
        print(f"  {key}. {value}")
    
    choice = input("è¯·é€‰æ‹© (é»˜è®¤1): ").strip() or "1"
    quantization = quantization_options.get(choice, "q4_k_m")
    
    print(f"\nğŸ“Š è½¬æ¢é…ç½®:")
    print(f"  æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"  è¾“å‡ºæ–‡ä»¶: {output_file}")
    print(f"  é‡åŒ–ç±»å‹: {quantization}")
    
    # 4. æ£€æŸ¥æ¨¡å‹ç±»å‹å¹¶å¤„ç†
    print("\n" + "=" * 50)
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºPEFTæ¨¡å‹
    if is_peft_model(model_path):
        print("ğŸ” æ£€æµ‹åˆ°PEFTæ¨¡å‹ï¼Œéœ€è¦å…ˆåˆå¹¶LoRAæƒé‡...")
        
        # ç”Ÿæˆåˆå¹¶åçš„æ¨¡å‹è·¯å¾„
        merged_dir = f"{model_path}-merged"
        if os.path.exists(merged_dir):
            print(f"âœ… å‘ç°å·²åˆå¹¶çš„æ¨¡å‹: {merged_dir}")
            model_path = merged_dir
        else:
            print("ğŸ“¦ å¼€å§‹åˆå¹¶LoRAæƒé‡...")
            if merge_lora_weights(model_path, merged_dir):
                model_path = merged_dir
            else:
                print("âŒ LoRAæƒé‡åˆå¹¶å¤±è´¥")
                return
    else:
        print("âœ… æ£€æµ‹åˆ°æ ‡å‡†æ¨¡å‹ï¼Œæ— éœ€åˆå¹¶")
    
    # 5. æ‰§è¡Œè½¬æ¢
    print("\n" + "=" * 50)
    if convert_to_gguf(model_path, output_file, quantization):
        print("âœ… è½¬æ¢æˆåŠŸï¼")
        
        # 6. åˆ›å»ºä½¿ç”¨ç¤ºä¾‹
        create_usage_script(output_file)
        
        # 7. æ˜¾ç¤ºç»“æœ
        print("\n" + "=" * 50)
        print("ğŸ‰ è½¬æ¢å®Œæˆï¼")
        print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
        print(f"ğŸ“ ä½¿ç”¨ç¤ºä¾‹: run_gguf_model.py")
        
        print("\nğŸ”§ ä½¿ç”¨æ–¹æ³•:")
        print("1. ç›´æ¥ä½¿ç”¨llama.cpp:")
        print(f"   ./llama.cpp/main -m {output_file} -n 512 -p 'ä½ çš„æç¤º'")
        print("\n2. ä½¿ç”¨ç¤ºä¾‹è„šæœ¬:")
        print("   python run_gguf_model.py")
        print("\n3. åœ¨LM Studioä¸­åŠ è½½:")
        print(f"   é€‰æ‹©æ–‡ä»¶: {output_file}")
        
    else:
        print("âŒ è½¬æ¢å¤±è´¥")

if __name__ == "__main__":
    main()

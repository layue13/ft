#!/usr/bin/env python3
"""
GGUFæ ¼å¼è½¬æ¢è„šæœ¬
å°†Hugging Faceæ¨¡å‹è½¬æ¢ä¸ºGGUFæ ¼å¼ï¼Œè·å¾—æœ€ä½³æ€§èƒ½å’Œå…¼å®¹æ€§
"""

import os
import sys
import subprocess
from pathlib import Path

def is_peft_model(model_path):
    """æ£€æŸ¥æ˜¯å¦ä¸ºPEFTæ¨¡å‹"""
    try:
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨adapter_config.json
        adapter_config = os.path.join(model_path, "adapter_config.json")
        if os.path.exists(adapter_config):
            return True
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨adapter_model.bin
        adapter_model = os.path.join(model_path, "adapter_model.bin")
        if os.path.exists(adapter_model):
            return True
        
        return False
    except:
        return False

def merge_lora_weights(model_path, output_dir):
    """åˆå¹¶LoRAæƒé‡"""
    print(f"ğŸ”§ æ£€æµ‹åˆ°PEFTæ¨¡å‹ï¼Œå¼€å§‹åˆå¹¶LoRAæƒé‡...")
    print(f"ğŸ“¦ æºæ¨¡å‹: {model_path}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer
        from peft import PeftModel
        
        print("ğŸ“¦ åŠ è½½åŸºç¡€æ¨¡å‹ (google/gemma-3-1b-it)...")
        base_model = AutoModelForCausalLM.from_pretrained(
            "google/gemma-3-1b-it",
            torch_dtype="auto",
            device_map="auto"
        )
        tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-1b-it")
        
        print("ğŸ“¦ åŠ è½½LoRAæƒé‡...")
        model = PeftModel.from_pretrained(base_model, model_path)
        
        print("ğŸ”§ åˆå¹¶æƒé‡...")
        merged_model = model.merge_and_unload()
        
        print("ğŸ’¾ ä¿å­˜åˆå¹¶åçš„æ¨¡å‹...")
        merged_model.save_pretrained(output_dir)
        tokenizer.save_pretrained(output_dir)
        
        print(f"âœ… LoRAæƒé‡åˆå¹¶å®Œæˆï¼")
        print(f"ğŸ“ åˆå¹¶åçš„æ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}")
        
        return True
        
    except Exception as e:
        print(f"âŒ åˆå¹¶å¤±è´¥: {e}")
        return False

def convert_to_gguf(model_path, output_file, quantization="q4_k_m"):
    """è½¬æ¢ä¸ºGGUFæ ¼å¼"""
    print(f"ğŸ”„ å¼€å§‹è½¬æ¢: {model_path} -> {output_file}")
    
    # æ£€æŸ¥llama.cppæ˜¯å¦å­˜åœ¨
    if not os.path.exists("llama.cpp"):
        print("ğŸ“¦ ä¸‹è½½llama.cpp...")
        try:
            subprocess.run([
                "git", "clone", "https://github.com/ggml-org/llama.cpp.git"
            ], check=True)
            print("âœ… å·²ä¸‹è½½llama.cpp")
        except subprocess.CalledProcessError as e:
            print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
            return False
    
    # æ£€æŸ¥è½¬æ¢è„šæœ¬
    convert_script = "llama.cpp/convert_hf_to_gguf.py"
    if not os.path.exists(convert_script):
        print(f"âŒ è½¬æ¢è„šæœ¬ä¸å­˜åœ¨: {convert_script}")
        return False
    
    try:
        # è¿è¡Œè½¬æ¢
        cmd = [
            sys.executable, convert_script,
            model_path,
            "--outfile", output_file,
            "--outtype", quantization
        ]
        
        print(f"ğŸš€ æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        subprocess.run(cmd, check=True)
        
        print(f"âœ… è½¬æ¢å®Œæˆ: {output_file}")
        return True
        
    except subprocess.CalledProcessError as e:
        print(f"âŒ è½¬æ¢å¤±è´¥: {e}")
        return False

def create_usage_script(output_file):
    """åˆ›å»ºä½¿ç”¨ç¤ºä¾‹è„šæœ¬"""
    print("ğŸ“ åˆ›å»ºä½¿ç”¨ç¤ºä¾‹è„šæœ¬...")
    
    usage_script = f"""
#!/usr/bin/env python3
\"\"\"
GGUFæ¨¡å‹ä½¿ç”¨ç¤ºä¾‹
\"\"\"

import os
import subprocess
import sys

def run_llama_cpp(model_path, prompt, max_tokens=512):
    \"\"\"ä½¿ç”¨llama.cppè¿è¡ŒGGUFæ¨¡å‹\"\"\"
    
    # å°è¯•å¤šç§è¿è¡Œæ–¹å¼
    commands = [
        # æ–¹å¼1: æœ¬åœ°llama.cpp
        ["./llama.cpp/main", "-m", model_path, "-n", str(max_tokens), "-p", prompt],
        # æ–¹å¼2: ç³»ç»Ÿå®‰è£…çš„llama.cpp
        ["llama-cpp", "-m", model_path, "-n", str(max_tokens), "-p", prompt],
    ]
    
    for cmd in commands:
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            return result.stdout.strip()
        except (subprocess.CalledProcessError, FileNotFoundError):
            continue
    
    print("âŒ æ‰€æœ‰è¿è¡Œæ–¹å¼éƒ½å¤±è´¥")
    return None

def main():
    model_path = "{output_file}"
    
    if not os.path.exists(model_path):
        print(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {{model_path}}")
        return
    
    # ç¤ºä¾‹æç¤º
    prompt = "å¸®æˆ‘æŸ¥è¯¢åŒ—äº¬çš„å¤©æ°”"
    
    print(f"ç”¨æˆ·: {{prompt}}")
    print("åŠ©æ‰‹: ", end="")
    
    response = run_llama_cpp(model_path, prompt)
    if response:
        print(response)
    else:
        print("ç”Ÿæˆå¤±è´¥")

if __name__ == "__main__":
    main()
"""
    
    with open("run_gguf_model.py", "w") as f:
        f.write(usage_script)
    
    print("âœ… ä½¿ç”¨ç¤ºä¾‹è„šæœ¬å·²åˆ›å»º: run_gguf_model.py")

def main():
    print("ğŸš€ GGUFæ ¼å¼è½¬æ¢å·¥å…·")
    print("=" * 40)
    
    # è·å–è¾“å…¥
    model_path = input("è¯·è¾“å…¥æ¨¡å‹è·¯å¾„ (ä¾‹å¦‚: ./gemma3-1b-tool-use-merged æˆ– layue13/gemma3-1b-tool-use): ").strip()
    if not model_path:
        model_path = "./gemma3-1b-tool-use-merged"
    
    output_file = input("è¯·è¾“å…¥è¾“å‡ºæ–‡ä»¶å (ä¾‹å¦‚: gemma3-1b-tool-use.gguf): ").strip()
    if not output_file:
        output_file = "gemma3-1b-tool-use.gguf"
    
    # é‡åŒ–é€‰é¡¹
    quantization_options = {
        "1": "q4_k_m",    # æ¨èï¼Œå¹³è¡¡å¤§å°å’Œæ€§èƒ½
        "2": "q8_0",      # é«˜è´¨é‡ï¼Œè¾ƒå¤§æ–‡ä»¶
        "3": "q5_k_m",    # ä¸­ç­‰è´¨é‡
        "4": "q3_k_m",    # å°æ–‡ä»¶ï¼Œè¾ƒä½è´¨é‡
    }
    
    print("\nğŸ”§ é€‰æ‹©é‡åŒ–ç±»å‹:")
    for key, value in quantization_options.items():
        print(f"  {key}. {value}")
    
    choice = input("è¯·é€‰æ‹© (é»˜è®¤1): ").strip() or "1"
    quantization = quantization_options.get(choice, "q4_k_m")
    
    print(f"\nğŸ“Š è½¬æ¢é…ç½®:")
    print(f"  æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"  è¾“å‡ºæ–‡ä»¶: {output_file}")
    print(f"  é‡åŒ–ç±»å‹: {quantization}")
    
    # æ£€æŸ¥æ¨¡å‹ç±»å‹å¹¶å¤„ç†
    print("\n" + "=" * 40)
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºPEFTæ¨¡å‹
    if is_peft_model(model_path):
        print("ğŸ” æ£€æµ‹åˆ°PEFTæ¨¡å‹ï¼Œéœ€è¦å…ˆåˆå¹¶LoRAæƒé‡...")
        
        # ç”Ÿæˆåˆå¹¶åçš„æ¨¡å‹è·¯å¾„
        merged_dir = f"{model_path}-merged"
        if os.path.exists(merged_dir):
            print(f"âœ… å‘ç°å·²åˆå¹¶çš„æ¨¡å‹: {merged_dir}")
            model_path = merged_dir
        else:
            print("ğŸ“¦ å¼€å§‹åˆå¹¶LoRAæƒé‡...")
            if merge_lora_weights(model_path, merged_dir):
                model_path = merged_dir
            else:
                print("âŒ LoRAæƒé‡åˆå¹¶å¤±è´¥")
                return
    else:
        print("âœ… æ£€æµ‹åˆ°æ ‡å‡†æ¨¡å‹ï¼Œæ— éœ€åˆå¹¶")
    
    # æ‰§è¡Œè½¬æ¢
    print("\n" + "=" * 40)
    if convert_to_gguf(model_path, output_file, quantization):
        print("âœ… è½¬æ¢æˆåŠŸï¼")
        
        # åˆ›å»ºä½¿ç”¨ç¤ºä¾‹
        create_usage_script(output_file)
        
        # æ˜¾ç¤ºç»“æœ
        print("\n" + "=" * 40)
        print("ğŸ‰ è½¬æ¢å®Œæˆï¼")
        print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
        print(f"ğŸ“ ä½¿ç”¨ç¤ºä¾‹: run_gguf_model.py")
        
        print("\nğŸ”§ ä½¿ç”¨æ–¹æ³•:")
        print("1. ç›´æ¥ä½¿ç”¨llama.cpp:")
        print(f"   ./llama.cpp/main -m {output_file} -n 512 -p 'ä½ çš„æç¤º'")
        print("\n2. ä½¿ç”¨ç¤ºä¾‹è„šæœ¬:")
        print("   python run_gguf_model.py")
        print("\n3. åœ¨LM Studioä¸­åŠ è½½:")
        print(f"   é€‰æ‹©æ–‡ä»¶: {output_file}")
        
    else:
        print("âŒ è½¬æ¢å¤±è´¥")

if __name__ == "__main__":
    main()

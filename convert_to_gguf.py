#!/usr/bin/env python3
"""
GGUFæ ¼å¼è½¬æ¢è„šæœ¬
ä½¿ç”¨llama.cppçš„convert_hf_to_gguf.pyå°†Hugging Faceæ¨¡å‹è½¬æ¢ä¸ºGGUFæ ¼å¼
æ”¯æŒPEFTæ¨¡å‹å’Œå¤šç§é‡åŒ–é€‰é¡¹
"""

import os
import sys
import subprocess
import shutil
from pathlib import Path

def check_dependencies():
    """æ£€æŸ¥å¿…è¦çš„ä¾èµ–"""
    print("ğŸ” æ£€æŸ¥ä¾èµ–...")
    
    # æ£€æŸ¥git
    try:
        subprocess.run(["git", "--version"], check=True, capture_output=True)
        print("âœ… git")
    except (subprocess.CalledProcessError, FileNotFoundError):
        print("âŒ git æœªå®‰è£…")
        return False
    
    # æ£€æŸ¥make
    try:
        subprocess.run(["make", "--version"], check=True, capture_output=True)
        print("âœ… make")
    except (subprocess.CalledProcessError, FileNotFoundError):
        print("âŒ make æœªå®‰è£…")
        return False
    
    return True

def install_llama_cpp():
    """å®‰è£…llama.cpp"""
    print("ğŸ“¦ å®‰è£…llama.cpp...")
    
    if os.path.exists("llama.cpp"):
        print("âœ… llama.cppå·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")
        return True
    
    try:
        # å…‹éš†llama.cppä»“åº“
        subprocess.run([
            "git", "clone", "https://github.com/ggml-org/llama.cpp.git"
        ], check=True)
        print("âœ… å·²å…‹éš†llama.cppä»“åº“")
        
        # ç¼–è¯‘
        os.chdir("llama.cpp")
        subprocess.run(["make"], check=True)
        print("âœ… å·²ç¼–è¯‘llama.cpp")
        os.chdir("..")
        
        return True
        
    except subprocess.CalledProcessError as e:
        print(f"âŒ å®‰è£…å¤±è´¥: {e}")
        return False

def convert_to_gguf(model_path, output_file, quantization="q4_k_m"):
    """è½¬æ¢ä¸ºGGUFæ ¼å¼"""
    print(f"ğŸ”„ å¼€å§‹è½¬æ¢: {model_path} -> {output_file}")
    
    if not os.path.exists("llama.cpp"):
        print("âŒ llama.cppæœªå®‰è£…ï¼Œè¯·å…ˆè¿è¡Œå®‰è£…")
        return False
    
    # æ£€æŸ¥è½¬æ¢è„šæœ¬
    convert_script = "llama.cpp/convert_hf_to_gguf.py"
    if not os.path.exists(convert_script):
        print(f"âŒ è½¬æ¢è„šæœ¬ä¸å­˜åœ¨: {convert_script}")
        return False
    
    try:
        # è¿è¡Œè½¬æ¢
        cmd = [
            sys.executable, convert_script,
            model_path,
            "--outfile", output_file,
            "--outtype", quantization
        ]
        
        print(f"ğŸš€ æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        subprocess.run(cmd, check=True)
        
        print(f"âœ… è½¬æ¢å®Œæˆ: {output_file}")
        return True
        
    except subprocess.CalledProcessError as e:
        print(f"âŒ è½¬æ¢å¤±è´¥: {e}")
        return False

def create_usage_script(output_file):
    """åˆ›å»ºä½¿ç”¨ç¤ºä¾‹è„šæœ¬"""
    print("ğŸ“ åˆ›å»ºä½¿ç”¨ç¤ºä¾‹è„šæœ¬...")
    
    usage_script = f"""
#!/usr/bin/env python3
\"\"\"
GGUFæ¨¡å‹ä½¿ç”¨ç¤ºä¾‹
\"\"\"

import subprocess
import sys

def run_llama_cpp(model_path, prompt, max_tokens=512):
    \"\"\"ä½¿ç”¨llama.cppè¿è¡ŒGGUFæ¨¡å‹\"\"\"
    cmd = [
        "./llama.cpp/main",
        "-m", model_path,
        "-n", str(max_tokens),
        "-p", prompt,
        "--repeat_penalty", "1.1",
        "--temp", "0.7"
    ]
    
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        return result.stdout.strip()
    except subprocess.CalledProcessError as e:
        print(f"è¿è¡Œå¤±è´¥: {{e}}")
        return None

def main():
    model_path = "{output_file}"
    
    if not os.path.exists(model_path):
        print(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {{model_path}}")
        return
    
    # ç¤ºä¾‹æç¤º
    prompt = "å¸®æˆ‘æŸ¥è¯¢åŒ—äº¬çš„å¤©æ°”"
    
    print(f"ç”¨æˆ·: {{prompt}}")
    print("åŠ©æ‰‹: ", end="")
    
    response = run_llama_cpp(model_path, prompt)
    if response:
        print(response)
    else:
        print("ç”Ÿæˆå¤±è´¥")

if __name__ == "__main__":
    main()
"""
    
    with open("run_gguf_model.py", "w") as f:
        f.write(usage_script)
    
    print("âœ… ä½¿ç”¨ç¤ºä¾‹è„šæœ¬å·²åˆ›å»º: run_gguf_model.py")

def main():
    print("ğŸš€ GGUFæ ¼å¼è½¬æ¢å·¥å…·")
    print("=" * 50)
    
    # 1. æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        print("\nâŒ ä¾èµ–æ£€æŸ¥å¤±è´¥ï¼Œè¯·å®‰è£…å¿…è¦çš„å·¥å…·")
        return
    
    # 2. å®‰è£…llama.cpp
    if not install_llama_cpp():
        print("\nâŒ llama.cppå®‰è£…å¤±è´¥")
        return
    
    # 3. è·å–è¾“å…¥
    print("\nğŸ“ é…ç½®è½¬æ¢å‚æ•°:")
    
    model_path = input("è¯·è¾“å…¥æ¨¡å‹è·¯å¾„ (ä¾‹å¦‚: ./gemma3-1b-tool-use-merged æˆ– layue13/gemma3-1b-tool-use): ").strip()
    if not model_path:
        model_path = "./gemma3-1b-tool-use-merged"
    
    output_file = input("è¯·è¾“å…¥è¾“å‡ºæ–‡ä»¶å (ä¾‹å¦‚: gemma3-1b-tool-use.gguf): ").strip()
    if not output_file:
        output_file = "gemma3-1b-tool-use.gguf"
    
    # é‡åŒ–é€‰é¡¹
    quantization_options = {
        "1": "q4_k_m",    # æ¨èï¼Œå¹³è¡¡å¤§å°å’Œæ€§èƒ½
        "2": "q8_0",      # é«˜è´¨é‡ï¼Œè¾ƒå¤§æ–‡ä»¶
        "3": "q5_k_m",    # ä¸­ç­‰è´¨é‡
        "4": "q3_k_m",    # å°æ–‡ä»¶ï¼Œè¾ƒä½è´¨é‡
    }
    
    print("\nğŸ”§ é€‰æ‹©é‡åŒ–ç±»å‹:")
    for key, value in quantization_options.items():
        print(f"  {key}. {value}")
    
    choice = input("è¯·é€‰æ‹© (é»˜è®¤1): ").strip() or "1"
    quantization = quantization_options.get(choice, "q4_k_m")
    
    print(f"\nğŸ“Š è½¬æ¢é…ç½®:")
    print(f"  æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"  è¾“å‡ºæ–‡ä»¶: {output_file}")
    print(f"  é‡åŒ–ç±»å‹: {quantization}")
    
    # 4. æ‰§è¡Œè½¬æ¢
    print("\n" + "=" * 50)
    if convert_to_gguf(model_path, output_file, quantization):
        print("âœ… è½¬æ¢æˆåŠŸï¼")
        
        # 5. åˆ›å»ºä½¿ç”¨ç¤ºä¾‹
        create_usage_script(output_file)
        
        # 6. æ˜¾ç¤ºç»“æœ
        print("\n" + "=" * 50)
        print("ğŸ‰ è½¬æ¢å®Œæˆï¼")
        print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
        print(f"ğŸ“ ä½¿ç”¨ç¤ºä¾‹: run_gguf_model.py")
        
        print("\nğŸ”§ ä½¿ç”¨æ–¹æ³•:")
        print("1. ç›´æ¥ä½¿ç”¨llama.cpp:")
        print(f"   ./llama.cpp/main -m {output_file} -n 512 -p 'ä½ çš„æç¤º'")
        print("\n2. ä½¿ç”¨ç¤ºä¾‹è„šæœ¬:")
        print("   python run_gguf_model.py")
        print("\n3. åœ¨LM Studioä¸­åŠ è½½:")
        print(f"   é€‰æ‹©æ–‡ä»¶: {output_file}")
        
    else:
        print("âŒ è½¬æ¢å¤±è´¥")

if __name__ == "__main__":
    main()

#!/usr/bin/env python3
"""
GGUFæ ¼å¼è½¬æ¢è„šæœ¬
å°†åˆå¹¶åçš„æ¨¡å‹è½¬æ¢ä¸ºGGUFæ ¼å¼ï¼Œè·å¾—æœ€ä½³æ€§èƒ½å’Œå…¼å®¹æ€§
"""

import os
import sys
import subprocess
from pathlib import Path

def check_dependencies():
    """æ£€æŸ¥å¿…è¦çš„ä¾èµ–"""
    print("ğŸ” æ£€æŸ¥ä¾èµ–...")
    
    try:
        import transformers
        print("âœ… transformers")
    except ImportError:
        print("âŒ transformers æœªå®‰è£…")
        return False
    
    try:
        # æ£€æŸ¥llama-cpp-python (ç”¨äºGGUFæ¨ç†)
        import llama_cpp
        print("âœ… llama-cpp-python")
    except ImportError:
        print("âŒ llama-cpp-python æœªå®‰è£…")
        return False
    
    try:
        # æ£€æŸ¥ctransformers (ç”¨äºGGUFè½¬æ¢)
        import ctransformers
        print("âœ… ctransformers")
    except ImportError:
        print("âŒ ctransformers æœªå®‰è£…")
        return False
    
    return True

def install_dependencies():
    """å®‰è£…å¿…è¦çš„ä¾èµ–"""
    print("ğŸ“¦ å®‰è£…ä¾èµ–...")
    
    dependencies = [
        "llama-cpp-python",
        "ctransformers[cuda]"  # æ”¯æŒCUDAåŠ é€Ÿ
    ]
    
    for dep in dependencies:
        try:
            # ä½¿ç”¨uv pipå®‰è£…
            subprocess.check_call(["uv", "pip", "install", dep])
            print(f"âœ… {dep} å®‰è£…æˆåŠŸ")
        except subprocess.CalledProcessError:
            print(f"âŒ {dep} å®‰è£…å¤±è´¥")
            return False
    
    return True

def convert_to_gguf(model_path, output_file, quantization="q4_k_m"):
    """è½¬æ¢ä¸ºGGUFæ ¼å¼"""
    print(f"ğŸ”„ å¼€å§‹GGUFè½¬æ¢...")
    print(f"ğŸ“¦ æºæ¨¡å‹: {model_path}")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
    print(f"ğŸ”§ é‡åŒ–ç±»å‹: {quantization}")
    
    # æ£€æŸ¥æºæ¨¡å‹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        print(f"âŒ æºæ¨¡å‹ä¸å­˜åœ¨: {model_path}")
        return False
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„æ–‡ä»¶
    required_files = ["config.json", "tokenizer.json"]
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
    model_files = ["pytorch_model.bin", "model.safetensors"]
    has_model_file = any(os.path.exists(os.path.join(model_path, f)) for f in model_files)
    
    missing_files = [f for f in required_files if not os.path.exists(os.path.join(model_path, f))]
    
    if missing_files:
        print(f"âŒ æºæ¨¡å‹ç¼ºå°‘å¿…è¦æ–‡ä»¶: {missing_files}")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œ merge_lora.py åˆå¹¶LoRAæƒé‡")
        return False
    
    if not has_model_file:
        print(f"âŒ æºæ¨¡å‹ç¼ºå°‘æ¨¡å‹æ–‡ä»¶ï¼Œéœ€è¦ä»¥ä¸‹ä¹‹ä¸€: {model_files}")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œ merge_lora.py åˆå¹¶LoRAæƒé‡")
        return False
    
    print("âœ… æ¨¡å‹æ–‡ä»¶æ£€æŸ¥é€šè¿‡")
    
    try:
        # ä½¿ç”¨transformersåŠ è½½ï¼Œç„¶åè½¬æ¢ä¸ºGGUF
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        print("ğŸ”„ ä½¿ç”¨transformersåŠ è½½æ¨¡å‹...")
        
        # åŠ è½½æ¨¡å‹å’Œtokenizer
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype="auto",
            device_map="auto"
        )
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        print("ğŸ”„ è½¬æ¢ä¸ºGGUFæ ¼å¼...")
        
        # ä½¿ç”¨llama.cppçš„è½¬æ¢å·¥å…·
        try:
            # å°è¯•ä½¿ç”¨llama.cppçš„convert.py
            convert_script = """
import sys
import os
sys.path.append('llama.cpp')

from convert import convert_hf_to_gguf

# è½¬æ¢æ¨¡å‹
convert_hf_to_gguf(
    model_path='{model_path}',
    output_path='{output_file}',
    model_type='llama'
)
"""
            
            # æ£€æŸ¥æ˜¯å¦æœ‰llama.cpp
            if not os.path.exists("llama.cpp"):
                print("ğŸ“¦ ä¸‹è½½llama.cpp...")
                subprocess.check_call(["git", "clone", "https://github.com/ggerganov/llama.cpp.git"])
            
            # è¿è¡Œè½¬æ¢
            with open("temp_convert.py", "w") as f:
                f.write(convert_script.format(model_path=model_path, output_file=output_file))
            
            subprocess.check_call([sys.executable, "temp_convert.py"])
            os.remove("temp_convert.py")
            
        except Exception as e:
            print(f"llama.cppè½¬æ¢å¤±è´¥: {e}")
            print("ğŸ”„ å°è¯•ä½¿ç”¨transformersç›´æ¥ä¿å­˜...")
            
            # å¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥ä¿å­˜ä¸ºtransformersæ ¼å¼ï¼Œç„¶åæ‰‹åŠ¨è½¬æ¢
            temp_dir = "./temp_model_for_gguf"
            model.save_pretrained(temp_dir)
            tokenizer.save_pretrained(temp_dir)
            
            print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°ä¸´æ—¶ç›®å½•: {temp_dir}")
            print("ğŸ’¡ è¯·æ‰‹åŠ¨ä½¿ç”¨llama.cppè½¬æ¢:")
            print(f"   cd llama.cpp")
            print(f"   python convert.py {temp_dir} --outfile {output_file} --outtype {quantization}")
            return False
        
        print("âœ… GGUFè½¬æ¢æˆåŠŸï¼")
        
        # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
        if os.path.exists(output_file):
            file_size = os.path.getsize(output_file) / (1024 * 1024)  # MB
            print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
            print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
        
        return True
            
    except Exception as e:
        print(f"âŒ è½¬æ¢å¤±è´¥: {e}")
        print("ğŸ’¡ æç¤º: å¦‚æœè½¬æ¢å¤±è´¥ï¼Œå¯ä»¥å°è¯•ä½¿ç”¨llama.cppæ‰‹åŠ¨è½¬æ¢")
        return False

def create_usage_script(output_file):
    """åˆ›å»ºä½¿ç”¨è„šæœ¬"""
    print("ğŸ“ åˆ›å»ºä½¿ç”¨è„šæœ¬...")
    
    script_content = f"""#!/usr/bin/env python3
\"\"\"
GGUFæ¨¡å‹ä½¿ç”¨ç¤ºä¾‹
\"\"\"

import os
from llama_cpp import Llama

def load_gguf_model(model_path):
    \"\"\"åŠ è½½GGUFæ¨¡å‹\"\"\"
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {{model_path}}")
        return None
    
    try:
        # åŠ è½½æ¨¡å‹
        llm = Llama(
            model_path=model_path,
            n_ctx=4096,  # ä¸Šä¸‹æ–‡é•¿åº¦
            n_threads=4,  # CPUçº¿ç¨‹æ•°
            n_gpu_layers=0  # GPUå±‚æ•°ï¼Œæ ¹æ®ä½ çš„GPUè°ƒæ•´
        )
        print("âœ… GGUFæ¨¡å‹åŠ è½½æˆåŠŸ")
        return llm
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {{e}}")
        return None

def generate_tool_call(llm, prompt, max_tokens=512):
    \"\"\"ç”Ÿæˆå·¥å…·è°ƒç”¨å“åº”\"\"\"
    try:
        # æ ¼å¼åŒ–è¾“å…¥
        formatted_prompt = f"<bos><start_of_turn>user\\n{{prompt}}<end_of_turn>\\n<start_of_turn>model\\n"
        
        # ç”Ÿæˆå“åº”
        response = llm(
            formatted_prompt,
            max_tokens=max_tokens,
            temperature=0.7,
            top_p=0.9,
            stop=["<eos>", "</s>"]
        )
        
        return response['choices'][0]['text']
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¤±è´¥: {{e}}")
        return None

if __name__ == "__main__":
    # åŠ è½½æ¨¡å‹
    model_path = "{output_file}"
    llm = load_gguf_model(model_path)
    
    if llm:
        # æµ‹è¯•å·¥å…·è°ƒç”¨
        prompt = "å¸®æˆ‘æŸ¥è¯¢åŒ—äº¬çš„å¤©æ°”"
        print(f"\\nç”¨æˆ·: {{prompt}}")
        
        response = generate_tool_call(llm, prompt)
        if response:
            print(f"åŠ©æ‰‹: {{response}}")
        else:
            print("âŒ ç”Ÿæˆå¤±è´¥")
"""
    
    script_file = "gguf_inference.py"
    with open(script_file, "w") as f:
        f.write(script_content)
    
    print(f"âœ… ä½¿ç”¨è„šæœ¬å·²åˆ›å»º: {script_file}")

def main():
    print("ğŸš€ GGUFæ ¼å¼è½¬æ¢å·¥å…·")
    print("=" * 50)
    
    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        print("\nğŸ“¦ å®‰è£…ç¼ºå¤±çš„ä¾èµ–...")
        if not install_dependencies():
            print("âŒ ä¾èµ–å®‰è£…å¤±è´¥")
            return
        if not check_dependencies():
            print("âŒ ä¾èµ–æ£€æŸ¥å¤±è´¥")
            return
    
    print("\n" + "=" * 50)
    
    # è·å–è¾“å…¥
    model_path = input("è¯·è¾“å…¥æ¨¡å‹è·¯å¾„ (ä¾‹å¦‚: ./gemma3-1b-tool-use-merged): ").strip()
    if not model_path:
        print("âŒ æ¨¡å‹è·¯å¾„ä¸èƒ½ä¸ºç©º")
        return
    
    output_file = input("è¯·è¾“å…¥è¾“å‡ºæ–‡ä»¶ (é»˜è®¤: ./gemma3-1b-tool-use.gguf): ").strip()
    if not output_file:
        output_file = "./gemma3-1b-tool-use.gguf"
    
    # é‡åŒ–é€‰é¡¹
    print("\nğŸ”§ é€‰æ‹©é‡åŒ–ç±»å‹:")
    print("1. q4_k_m (æ¨è) - å¹³è¡¡è´¨é‡å’Œå¤§å°")
    print("2. q8_0 - é«˜è´¨é‡ï¼Œè¾ƒå¤§æ–‡ä»¶")
    print("3. q5_k_m - ä¸­ç­‰è´¨é‡")
    print("4. q3_k_m - å°æ–‡ä»¶ï¼Œè´¨é‡è¾ƒä½")
    print("5. æ— é‡åŒ– - ä¿æŒåŸå§‹ç²¾åº¦")
    
    quantization = input("è¯·é€‰æ‹©é‡åŒ–ç±»å‹ (é»˜è®¤: q4_k_m): ").strip()
    if not quantization:
        quantization = "q4_k_m"
    
    # æ³¨æ„ï¼šctransformersçš„é‡åŒ–é€‰é¡¹å¯èƒ½ä¸åŒ
    print(f"ğŸ’¡ æ³¨æ„: ä½¿ç”¨ctransformersè¿›è¡Œè½¬æ¢ï¼Œé‡åŒ–é€‰é¡¹: {quantization}")
    
    print("\n" + "=" * 50)
    
    # æ‰§è¡Œè½¬æ¢
    if convert_to_gguf(model_path, output_file, quantization):
        print("\nğŸ‰ GGUFè½¬æ¢æˆåŠŸï¼")
        print(f"ğŸ“ GGUFæ–‡ä»¶: {output_file}")
        
        # åˆ›å»ºä½¿ç”¨è„šæœ¬
        create_usage_script(output_file)
        
        print("\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
        print("1. å®‰è£…ä¾èµ–: pip install llama-cpp-python")
        print("2. è¿è¡Œç¤ºä¾‹: python gguf_inference.py")
        print("3. åœ¨LM Studioä¸­åŠ è½½GGUFæ–‡ä»¶")
        print("4. ä½¿ç”¨llama.cppè¿›è¡Œæ¨ç†")
    else:
        print("\nâŒ GGUFè½¬æ¢å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")

if __name__ == "__main__":
    main()
